# Pat Casey Tech Journal

## Pat Casey on event management architecture in the core platform
I thought I might focus on a fairly concrete, technical problem, which is how the ServiceNow event processing system works. For clarity I’m not talking about our event management product, I’m rather talking about the event processing service inside of the platform where, for example, business rules trigger events, and the events in turn are processed and emails sent out. It’s a surprisingly complicated subsystem that has a lot of history behind it, so I thought it might be interesting to this audience to drill into a bit.

The first thing to understand is why we have an event system at all. Why does sending an email inside of ServiceNow a two part dance where a business rule fires an event, and then a totally different piece of code (and event handler) reacts to that event and actually sends an email. Wouldn’t it, after all, be easier to just have the business rule send the email directly? Or at least construct the email and put it into some sort of outbound queue for subsequent delivery? The short answer is, of course, that it *would* be easier for the person writing the business rule, and it *would* be easier for the people writing the platform, but it would, nonetheless, be a bad idea.

That kind of direct coupling is a bad idea because it means that whenever you want to change the email that goes out, you are also changing the business rule that triggers the email. Fred and I had both worked extensively with workflow systems in the past where there was no separation between the concepts of “trigger” and “action” which means that changing either one de-facto changed both. It led to fragile, really hard to upgrade, code for the customers. In practice it also meant that the code which triggered the action also had to incur the cost (and the time budget) of executing the action itself, which tended to slow down things like submitting incidents. So with ServiceNow, we decided we were going to do something different. We were going to communicate between subsystems via events, rather than explicit coupling, and the event system was created to handle this kind of separation.

The first iteration of the event system was brutally simple, because the product as a whole started out brutally simple. Events were nothing more than rows that got inserted into a table called sysevent. There was then a single scheduled job that woke up periodically (every 60 seconds if I recall) and would query the sysevent table for all rows with a process state of “ready”. It would then iteratively work through those events, do whatever was required, and update their process state to “processed” when it was done. This approach worked well, and it even worked well in a cluster since our scheduler would ensure that there was only one instance of the event processing job running at a time across the cluster, so multiple threads on multiple JVMs could enqueue events by inserting rows, and then one central event processing job would process all the events. 

Like most simple algorithms, it worked until it didn’t. For our largest customers, we started to see problems where the central event processing thread couldn’t keep up. There were simply too many events being generated for that single thread to process them all and so we started seeing eventqueues balloon during peak business hours. Naturally this made the customers sad since they’d enqueue an event that should logically result in an email going out, and then that email wouldn’t get sent for an hour or so. We did the usual stuff of course, we optimized the code, we tried to get the customers to write less events, etc. It bought us some time, but eventually we had to switch our approach.

The obvious solution, of course, is to have multiple event processors. If one thread cannot keep up, perhaps two will, or four. Just naively firing up four copies of the event processor job though would introduce really nasty logic errors though as all the processors would query the same list of “ready” events at once, and each events would then get processed four times in parallel. Customer would, quite literally, get four emails. So to make multiple event processors work, we had to move away from the simple “query and process iteratively” approach to something a bit more sophisticated.

The shift we made was to move to a *claiming* model where each event processing job would start off by updating any events in the system in a state of ready, and setting them to something that logically meant “this event is claimed by processor #1”. Processor #1 would then follow up its claiming move by querying back those jobs that it had just claimed and process them sequentially. This approach relies on the database we are using being atomic for updates, but the databases we use all tick that box so it worked well.

This approach let us scale the event processing system really well, but it had a couple of side effects. One side effect is that events now had the possibility of being executed materially out of order. Event processor #1 could claim a mess of events generated between 12:00 and 12:01 and start working on them. Event processor #2 could then fire up and claim all events generated between 12:01 and 12:02. In a whole lot of cases, event processor #1 would still be working on an event from like 12:00:15 at the time event processor #2 started working on an event generated at 12:01:00. Now technically the event processing subsystem never guaranteed in order execution, but it was something people had come to expect. It’s also a problem we never really solved, we just sort of shrugged and told people “yeah, events will process sort of in order but not precisely, deal with it”.

There was, however, a second more insidious problem with the claiming approach. If events are being generated at a steady rate, and our event processors fire every 15 seconds or so, then each one will claim about 15 seconds worth of events so the load is reasonably equitably distributed across the processor. If you imagine a scenario though where a very large number of events are fired off in a short period of time, they will all get claimed by the next event processor that starts up. That job could plausibly claim like 10,000 events (which would take it like an hour to work through), leaving none available for the other event processors to work on. They, in turn, would keep processing the new events that came in after the surge, but the surge of events would land on one and only one event processor which, again, made the customers sad since their emails took too long to send.

To fix that particular problem, we had to go a more sophisticated claiming model. These days the events are not claimed such much as they are distributed. One node in the cluster is algorithmically selected to claim the nodes on behalf of all the other event processors. A job runs in that node, in particular, and it claims “chunks” of 500 events at a time on behalf of each of the known eventprocessors. So if like 10,000 events show up, and we have 4 event processors, the delegator will claim 500 on behalf of Processor #1, 500 on behalf of processor #2, 500 on behalf of processor #3, and 500 on behalf of processor #4, and then it will repeat itself until it runs out of events.

The point of all this, I suppose, is really twofold. One is that I thought people might like to understand how the event processor works, since its pretty fundamental to the way Glide/ServiceNow operates these days. The second point I wanted to make though was that the algorithms we use to solve problems are going to be tied to the scale we want to operate over. Things which work great at low scale break down as the scale goes up, and conversely really sophisticated scalable algorithms can be buggy, hard to implement, overkill for lots of simple problems e.g. we don’t use map-reduce to sort a couple of hundred strings.

End of the day, it’s about picking the right algorithm for the scale of the problem you have, but being flexible enough to change it if necessary.

Code, like everything else, evolves to meet the needs of its environment,

--- Pat


## Pat Casey on our app nodes cluster
In the early days of ServiceNow, our product ran on one and only one JVM, and talked to one and only one database. That’s still the way most of you run your development environments, and it’s still the way we run like the developer instances in the farm. From an engineering standpoint this makes a lot of problems really easy. Since the only JVM messing with the database is your JVM, you can make strong assumptions like “if I didn’t change it in the DB, it didn’t change in the DB”. Likewise a whole bunch of communication, caching, queuing and sequencing problems are trivially solved. There is, however, a really big problem with this approach, and that is scale and redundancy. At some point load on the system exceeds what one JVM can do, and at some point JVMs crash, and we don’t want to take the customer offline every time a JVM crashes. So maybe 2 years into the company’s lifespan, Fred rolled his chair over to the spot on the table I was working and said something to the effect of “we really need to be a cluster”, which was what passed for a product definition document in those days.

There were, of course, a number of specific problems I had to solve to transition us from a single JVM system to a clustered system. There was cache management and invalidation which is to say if I cache something on JVM1, and change the underlying data on JVM2, we need to invalidate the cache on JVM1. There was also locking, which is to say that if there was something that should only be happening once across the cluster, we needed to have JVM1 lock that resource so that JVM2 couldn’t access it until JVM1 released the lock. Finally there was distribution, which is to say that if traffic arrived in our datacenter we needed an approach to distribute it across the cluster.

Now in theory, cache invalidation is trivially easy. You put stuff in cache, and then when it’s no longer valid, you expire/remove it. In a cluster though this gets harder since every JVM has its own cache, and they need to communicate with each other when they expire stuff. You could imagine some sort of globally shared cache that all the JVMs connect to, but A) the network hop is too slow for many of our cache use cases and B) it doesn’t make the invalidation problem go away, it just moves it off to the side. So we went with a per JVM cache, but we still needed an invalidation mechanism. To provide that, there is a table in the database called sys_cache_flush. Every time we change a piece of data which might plausibly cause a cache to invalidate (deliberate oversimplification) we write a row into sys_cache_flush with an ever increasing sequence number. Every time any transaction (or background job) starts anywhere in the cluster, in looks for “new” entries in sys_cache_flush and processes them by flushing any listed entries out of the cache.

In general this approach works well for us, but it does have a couple of subtleties. One is, unsurprisingly, that if you have a piece of data which is both cached and changing frequently, you will generate a lot of entries in sys_cache_flush and bog down the system doing nothing but cache invalidation. With a different flush implementation, like doing it all via a shared memory queue or something, we could probably drive more invalidation traffic before we slowed down the system, but we’d still run into a limit eventually. Net-net on this one is that it’s not really appropriate to cache anything that changes super frequently in this (or any other) system. The other risk is more theoretical than real, and that is that the cache is flushed on transaction start, not on invalidation. So if you invalidate something on your JVM and I have a transaction already in progress, my cache isn’t going to flush until another thread on my JVM starts a new transaction. Which means there is a small time slice during which I will run with an invalidly cached entry. Again we’ll have the same thing regardless of mechanism we use for cache flush, but it’s something to be aware of.

The second problem we had to solve though was locking. For clarity’s sake, we’re talking about logical locking here e.g. the need to logically prevent two different pieces of code doing the same thing at the same time in the cluster. We are not talking about physical locking like the lock management within the database to avoid concurrency conflicts. That raises a problem though of how do you lock something across a cluster without some form of shared location to store the locks? For Glide, we already had a shared location in that all the JVMs talked to the database and it was operationally *way* simpler to use the database for locking than it would have been to roll out some new piece of kit in the datacenters to track locks. So the question then becomes, how to implement application layer locks in a database? Databases lock stuff all the time inside of their own memory space, but they don’t universally expose locking primitives to application (there’s no ANSI SQL “lock resource” statement for example). So we needed to build an abstraction layer that let folks lock (and unlock) logical resources via a natural(ish) API point like “Lock.get(“badger”)”.

The solution to this was the Mutex class. Logically you invoke Mutex.java and ask for a lock on a named resource like “badger42”. When you invoke Mutex.get() one of two things will happen (warning, oversimplification) either A) it will return immediately letting you know you locked that resource or B) it will block until such time as that lock is available and then return once the lock is available. Behind the scenes Mutex implements this by inserting rows into a table with a unique constraint called sys_mutex. If I succeed in inserting a row into that table, I have gotten the lock. When I release the lock, I delete the row out of the table. If, instead, I try to insert the row into the table, and get a duplicate key error, then somebody else must have the lock and I’ll try to insert it again in a few milliseconds. In practice its more complex than that and we do a lot of things via updates rather than insert/delete but you get the idea. There’s also a parallel implementation called FastLock that we use for certain use cases that relies on MySQL/MariaDB’s built in get LOCK semantic, but it has enough side effects that it’s not a global replacement for Mutex.

Mutex, in turn, works out pretty well and we use them throughout our applications whenever we need to guarantee mutual exclusion on certain code. The scheduler uses it to ensure that only one node in the cluster is popping things off the scheduler queue, discovery uses it to ensure that only one discovery job at a time is updating a specific CI, and we expose it to scripting via the GlideLock class. One side effect of the mutexes though is that when you get/release a mutex you end up doing a LOT of database activity on the sys_mutex table. In practice this doesn’t matter very much as databases are really good at inserting/updating/deleting rows. It does have the risk of bogging down our replication traffic though so we commonly blacklist sys_mutex in our binlog replication model. If we have to fail you over to the other DC and restart your whole cluster all the locks will get auto-reset anyway.

Distribution sounds like it ought to be easy to solve, but it’s actually one of the more subtle problems in a cluster. You could imagine that we would just spray incoming transactions randomly around on the JVMs. This has a side effect though that as your transactions bounce around the cluster, each JVM node in turn has to know who you are, maintain a session for you, and to use some of its metadata cache space for the artifacts you happen to frequently use. So to make the system more efficient, we ended up implementing session affinity, which is to say that when your first transaction arrives at the load balancer we randomly (sic) pick a JVM to send your traffic to AND stick a special routing cookie into the response. The load balancer, in turn, sees that routing cookie and routes subsequent requests from you to that same JVM. We used to have the JVM’s stick the routing cookies in and just have the load balancers read them, but it turned out to be easier to just have the load balancers do all the work (plus we bought more sophisticated load balancers).

This approach of session affinity helps with operational efficiency (since JVMs tend to get more consistent workload), but it does have a couple of side effects. One is that if your JVM crashes, the load balancer has to send you someplace else since it cannot honor the session affinity cookie. That new JVM in turn, won’t know who you are or anything about you, so that’s a problem. Historically, that JVM would just redirect you to the login screen since it doesn’t know who you are. These days there’s a session rebuild feature that can reconstruct your authentication status and who you are based on some information we serialize in the database when your transaction completes. It doesn’t get everything back (like the transaction stack we use to redirect you after submission), but it’ll at least know who you are.

If you know how the underlying cluster code works really well, you are definitely aware I oversimplified several of these explanations, but they’re directionally accurate. They key things to recognize about our cluster implementation is that it’s entirely based on the nodes communicating through the database. That makes some things harder, but it also make a lot of things dramatically easier especially in the world of operations.

For glide, the database really is the center of the universe.

--- Pat

## Pat Casey on Glide Scheduler
For today’s entry I thought I’d circle back to a technical topic and discuss how Glide’s background scheduler system works. From a logical standpoint, the scheduler actually tries to solve three related but not quite identical problems. First of all, it lets you, as a developer, schedule a piece of work for some future date. You can say, for example, Tuesday at 1:00 remind me to buy my wife flowers for mother’s day. Second, and *almost* the same, it lets you schedule a recurring piece of work that runs on some schedule on *all* future dates which meet its criteria e.g. every Tuesday at 1:00 remind me to take out the trash. Third, and most subtly, the scheduler is also a way for you to take a time consuming piece of work that is triggered by something the user did (like updating an incident) and executing in asynchronously, in another thread, but “more or less” now.

It turns out that you can accommodate all three of those use cases with one conceptual data structure encompassing three things.  There’s a trigger which is to say the time at which you want the event to occur, there’s a recurrence schedule which determines when subsequent instances of this event are supposed to occur (if any), and there’s an actual job you want to execute at that particular point(s) in time. Inside of glide, this is all encompassed on a table called, creatively, sys_trigger. The most important fields on sys_trigger, in turn, are next_action, which is the time at which a particular piece of work is next supposed to execute, job_id, which is a reference to the particular *class* of job that is supposed to fire, and job_context which is the specific context about this, unique, job.

As an example, you might have a job that is supposed to write the words “hello world” into the log at noon on Christmas 2020. It would have a next_action of 12/25/2020 12:00:00. It would have a job_id of “run javascript”. It would have a context of “gs.log(‘hello world’)”. There’s other ways you could do it of course; you could, for example, have a java class that did nothing but wrote the words hello world into the log, and then reference that *java* class directly from your sys_trigger record. In practice though, these days, almost all jobs use the default javascript class and just run script stored in the job_context field.

For purposes of today’s discussion we’re going to wave our hands over the concept of recurrence schedules. Suffice it to say though that some jobs recur on different schedules, and there is a data structure embedded within sys_trigger such that, when a job completes, it will reschedule itself properly for the next desired recurrence. Recurrence, in turn, is an interesting academic problem, but its pretty well solved and the code’s not that complicated (look up ARecurrance.java if you re really curious), so I’ll leave it as an exercise to the reader.

What I do want to focus on today though is distribution and scheduling which are much more complicated problems. You might imagine a naive algorithm where I dedicated one thread on the JVM to periodically run a query against sys_trigger, and then execute any pieces of work that it finds there which are scheduled to occur prior to “right now”. This actually works for simple cases, but it has a couple of flaws. One flaw is that it only works well in a single JVM system since otherwise two JVMs might schedule the same job at the same time. The other flaw is that you could imagine a bunch of jobs coming due at the same time and clogging up our one thread which (after all) can only do one job at a time. Since its sequential you could also imagine a failure mode where one poorly written job could take like an hour to run, bogging down the whole scheduler for an hour until it completes.

As a defense against the multiple threads problem, the scheduler doesn’t just naively query the sys_trigger table for work and then start executing it. Instead, a thread on each JVM will first acquire the scheduler mutex (which we discussed in our earlier article on cluster behavior). Only after it has acquired the mutex, guaranteeing that it is the only node in the cluster looking at the scheduler queue, will it query for active jobs. We want to avoid the starvation problem we articulated above, and we especially want to avoid holding the cluster-wide scheduler mutex open for any long than we have to, so the scheduler doesn’t do any of the jobs. All it does is mark the ones it has already queried as “queued” instead of “ready” and put them in a memory queue. Then it releases the mutex and goes back to sleep.

So now that we have some job contexts that we have pulled out of the scheduler queue and stuck into a memory queue, we need some threads to actually run them. The scheduler, in turn, has a configurable number of schedule worker threads (default is 8 per JVM if I recall) which pluck jobs off of the memory queue and actually execute them. Execution actually consists of a three part process. First the job is marked, in the database, as running. Second, the job is executed. Third, upon job completion, the job is either rescheduled and marked again for its future due date in the sys_trigger table, or the job is deleted if it doesn’t need to run again.

The early versions of the scheduler worked almost exactly as is described above, but there were a couple of subtle flaws/weaknesses in the approach. One flaw was that the scheduler subsystem was vulnerable to flooding. If you did something, like schedule a discovery, that queued up thousands of background jobs all at once, the scheduler subsystem would start working on the backlog and work it down over an hour or so, but while it was doing so it wouldn’t be running the normal care and feeding jobs that customers rely on like checking and sending email every sixty seconds. So during a flood the expected background jobs on the system would appear to be stuck. The second flaw was more subtle, and it’s the computer science version of a water hammer. When we as developers like to schedule things we tend to think in terms of stuff like “run this every night at 2:00 AM”. If *enough* developers are thinking the same way though there’s this huge nuclear weapon of jobs that are all scheduled to execute at *exactly* 2:00 AM. Not 1:59 AM, not 2:01 AM, exactly 2:00 AM and that, in turn, stresses out the system and creates its own flood.

Probably like most human beings who have written a really slick, sort of minimalist, piece of code that solves a hard problem *unless* people give it weird input conditions (like the two aforementioned flaws), we then spent about two years telling people not to give the scheduler bad input conditions. We worked with our internal developers. We worked with customers. We explain, we cajoled, we screamed, we yelled, we laughed, we cried, it was the full gambit of engineering denial. Eventually though, we sort of realized that, despite our best efforts, the world was not giving us perfect input conditions and hence we were going to have to adjust our algorithm.

To solve the narrow flooding problem, we introduced two defense mechanisms. The first is that jobs have *priority* now. Jobs with lower priority numbers will always queue prior to jobs with higher priority numbers (think of them like unix NICE values in that lower priority value == run me first). We then assigned really low priority numbers to jobs like the email reader or the upgrade scheduler guaranteeing that they would always find a place in the queue even during and flood. As a further defense the system will also now spin up an extra, transient, worker thread anytime a mission critical job (priority <= 25) has been sitting in the queue for more than 60 seconds. This combination of factors means that, these days, we rarely see an important job unable to run because of a flood of less important stuff.

The fix for the water hammer problem turned out to be simpler … jobs are now built to be deliberately inaccurate in their recurrence behavior. The recurrence algorithm is now configured to reschedule with a slight, configurable *error* in its recurrence schedule called max_drift. A job scheduled to execute every night at 2:00 AM with a max_drift of 10 minutes will now actually queue anytime between 1:55 AM and 2:05 AM e.g. correct time +- ½ max_drift. This, in turn, means that even in those cases where developers or customers like to align all their jobs on nice, neat one hour boundaries or at critical times during the night, the scheduler workload is actually spread out a bit temporally.

In practice there are some other features to the scheduler that we won’t go into here. There’s a built in serialization/deserialization feature that allows you to serialize job class member variables into the job context if you follow a field naming convention for example. There’s likewise a whole subsystem called the cluster reaper whose job it is to identify, and clear, jobs which have been claimed by cluster nodes which crashed or are otherwise offline. There was even a fairly advanced error handling subsystem that used to try to mark jobs as “buggy” and stop running them if they failed over and over again in a row (but we ended up turning that feature off since between us and our customers we had enough buggy jobs that nonetheless worked some of the time as to make this feature “break” a lot of jobs).

Point of all this though, I suppose, is that writing a good, at scale scheduler, is part science and part art. The first version we wrote was what I’d call a science scheduler. It was algorithmically correct and did what it was coded to do. It didn’t work perfectly though with the imperfect jobs and recurrence schedules that actual users threw at it though so we had to make adjustments to the algorithm (prioritization) and introduce a really subtle behind the scenes error to protect our users from themselves (drift). All in all though, it works well these days and is one of the subsystems I rarely see failing in customer use cases or scenarios (rarely != never mind you).

In my experience, if your scientifically correct algorithm doesn’t work well in the real world, the right approach is to make accommodations in the algorithm rather than trying to get the world to align with your approach.

For that matter, its probably good advice even outside of engineering :).

--- Pat


## Pat Casey on memory management and why our platform has its own date parser
So today I thought I’d talk a little bit about something that underlies almost everything we do on a modern computer. It’s something though that modern programming languages and environments have gotten so good at abstracting away though that it’s easy to ignore it and/or treat it like a black box. Nonetheless, understanding it will make you a better engineer, even if you can get away without the knowledge. What I’m talking about, is memory, and by extension, memory management. I’ll start off by saying that I’m going to make a number of over-simplifications here, so if you happen to have a PHD in compiler design or a really, really good understanding of how things like CPU cache pipelines work, you are going to find fault with a few things in today’s discussion. That being said, my goal isn’t to equip every reader here to write a compiler, but rather to understand how memory works at a high level to make their everyday engineering tasks better.

When we program, we typically think in terms of variables or objects. In java we simply construct an object, and it magically exists e.g. String s = “hello world”. Behind the scenes though, the runtime environment is doing a lot of work to create (and track) that string we just created. First of all, the JVM needs to figure out how big it’s going to be. In most JVMs strings are encoded as UTF8, and this particular string contains no unusual characters, so we can encode it with 1 byte per character which works out to 10 bytes of memory. On a modern operating system, memory is usually managed in 64 bit (8 byte) chunks so in order to hold our 10 bytes of memory we need to round it up to the nearest 64 bit boundary and hence we actually need 16 bytes of memory (sic).

Let’s make a gross oversimplification and assume that our JVM is just starting up and hence has no memory available at all. It has to request *more* from the operating system. You might imagine that the JVM would ask for 16 bytes of memory since that’s all we need. That would be wrong though; asking for memory takes time and puts a burden on the OS. So the JVM does not ask for fresh memory from the operating system every time it needs it. Instead it asks for it in big chunks, and then keeps track of the memory itself. For our purposes, lets assume the JVM asked for, and got, 128M of memory from the operating system.

What the JVM then does is *allocate* 16 bytes of the 128M of memory it just got. Allocation, in turn, is both less, and more, than it seems. The memory itself doesn’t change when it gets allocated. What changes is the way the JVM thinks about it. That particular chunk of memory is now marked as “in use” and hence is not available for any other allocation to use. Behind the scenes though it’s still just an address space in main memory with random noise in it. To actually get the words “hello world” into it we have to go through object initialization.

The first part of object initialization is straightforward. The JVM writes the bytes h e l l o w o r l d into the spot in memory that it just allocated. It needs some way for your program to actually *refer* to that memory though. You might want to, you know, do something with the string you just allocated. So the JVM doesn’t just need to build the string, it also needs to build an object pointer. Object pointers, in turn, and just addresses in main memory where various things can be found. In modern JVMs they are usually 8 bytes wide in order to reference all possible memory. In special cases though where your program is < 2G is space, the JVM can use shorter 4 byte pointers which end up saving a lot of memory. In our case though, let’s just assume it’s a standard 8 byte pointer and its pointing to the location in memory where the JVM just stored the bytes “hello world”.

In our program, we can now use the local variable s to refer to that point in memory. Because java is a typed language, the compiler knows not only that s is a reference to a spot in memory, but also knows that, in that spot in memory, it will find a String. In other languages (like C) with weaker typing, you can do wicked things with void pointers and treat the memory where you just stored the words hello world as, say, an array of integers if you really wanted to. In java though, its relatively hard to do truly evil things to the memory model because of the typing system. So from your programming point of view, you now have a logical token “s” that points to an address in memory where the bytes “hello world” are stored, and you can do stuff with it.

It’s worth pointing out that you can have more than one object pointer referencing the same spot in memory. The trivially easy way to do this is just by creating two variables e.g. String s = “hello world”; String t = s; In that fashion both t and s will both be pointing to the same memory address. String in java have a special property that they are immutable, which means that that java never changes the underlying string once allocated, instead it always makes a new string whenever you alter the one you started with. It means that every time you change a string you are actually generating a new one, but it also means that you won’t surprise yourself by having two pointers to the same string, changing the string via one of the pointers, and then reading it back from the original pointer and get a different value.

Now what we’ve talked about is actually the *easy* part of memory management. We allocated and used some memory. The problem becomes what to do with it when we don’t need it anymore? You could imagine that every time we created a new variable in java the JVM would allocate new memory for us, but if it did that approach it would eventually run out of available memory and every application would eventually crash with an OutOfMemory exception. So all programming languages need some way to deallocate memory that’s no longer in use. In the context of java that means that when you aren’t referencing your “hello world” string anymore, the JVM should be able to reclaim that memory and use it for something else.

That, turns out to be a really hard problem. In c, you as the programmer, were responsible for cleaning up your own memory. Anytime you allocated memory via malloc, you had to subsequently call free to let the operating system know that memory was no longer in use. Java hides that from you though by being a garbage collected language, which means that the JVM *does not* require you to free your memory. Instead the JVM will figure out if memory is no longer in use and mark it as free all on its own, and it’s really good at it. Most of the time, you can utterly ignore memory management on java and just let the garbage collector do its job, but most of the time is not the same as all of the time. So it helps to understand how garbage collection works.

The most basic question any garbage collection system has to answer is “is a particular piece of memory garbage or not”. In the context of java, that question can be re-written as “does any object pointer, anywhere in scope, currently point to this memory”? In our trivial example where we allocated a String called s, the underlying “hello world” is clearly being pointed to as long as our variable s is in scope. Imagine that we allocated the String inside of a function though and the function returned, meaning that s was no longer in scope. The underlying “hello world” memory is still allocated, still holds the words hello world, but is no longer reachable via any object pointer. Its garbage, but the JVM doesn’t know it yet.

To find the garbage, the JVM has to run some form of garbage collection algorithm. There are a lot of them, so I won’t describe them all, but the one we do run is the ConcurrentMarkSweep collector. That collector, in turn, operates by (warning oversimplified) first marking everything in memory as “possible garbage”. It then goes through and follows every object pointer it references and, for any memory address referenced by a pointer, it marks the memory from “possible garbage” to “not garbage”. Once it’s done following all the object pointers, anything that is still marked as “possible garbage” is now known to be garbage and hence can be freed. When objects are freed, in turn, all that really happens is that memory (however large it may be) is added to the JVM’s internal list of “free” memory that it can use for something else. Since the freed memory chunks are not likely to be contiguous and are of various sizes and lengths, the JVM keeps different free lists for different sizes of memory.

There are some flaws with the garbage collection approach I just outlined. One is that the naive implementation requires that we pause the JVM while we run garbage collection. I can’t very well run a mark/sweep approach is the memory is being changed while I’m marking it. In practice, the more modern ConcurrentMarkSweep approach can *usually* manage without locking the JVM but it does periodically have to trigger a major GC which can pause the JVM for a second or so under acute circumstances. The duration of that pause, in turn, is a function of how big the JVM is to start with so a larger JVM will have longer GC pauses.

The second flaw with this GC approach though is that it consumes a lot of CPU cycles. All that marking and sweeping requires your CPU to spend a lot of cycles on garbage collection instead of making your program work so it makes your program run slower. To limit the impact of this, most JVMs now use something called generational garbage collection. The idea behind that, in turn, is that most object are highly transient, and get allocated and then subsequently go out of scope within a single GC cycle. Object which manage to survive for longer though are probably cached, static, or otherwise highly referenced and are very likely to *keep living*. So garbage collectors will typically break up memory into three chunk, called younggen, oldgen, and permgen. Objects in YoungGen are garbage collected frequently, and if an object survives long enough there it is “promoted” to OldGen. Objects in OldGen, in turn, are GCd less frequently. You might assume that when objects survive for a long time in OldGen they get promoted to PermGen, or you might alternately assume that PermGen is, you know, permanent. Unfortunately neither is true. It turns out that PermGen is where java keeps the byte code for your classes, interned strings, and other quasi-internal data structures. It also turns out that java does garbage collect PermGen, usually during a major GC.

So why does this matter to you as a java programmer? The short version is that most of the time, it doesn’t. You can just write your program in whatever way is semantically most expressive, ignore object allocation and memory management, and spend your mental cycles on solving your business problem. There are, however, exceptions. There are certain algorithms that get run at very high frequency in the platform (and potentially in application code) where excess object generation is a problem. Excess garbage generation causes excess garbage collection which, in turn, slows your whole program down.

One of the classic examples is that we parse a LOT of dates in our product. Any time we pull any row out of the database, we parse at least two dates, sys_updated_on and sys_created_on. If you look in the bowels of the code though, you’ll find a class called GlideDateParser, and you might be thinking, why did we write our own Date parser? Doesn’t the JVM’s built in SimpleDateParser work? Well the short version is the SimpleDateParser throws off almost 1k of garbage every time it parses a date, and we parse so many dates that it made sense to write our own date parser. Similarly, there are certain strings that the platform sees over and over again, words like “true” or “one” or “system” where the platform uses classes like StringDeduper to ensure that if multiple parts of the platform need to allocate a string of “one” that we only allocate it once in memory and just have multiple object pointers referencing the same memory rather than like 10,000 copies of the word “one” uniquely allocated at 8 bytes each in memory.

Another place things matter is more in the context of not writing bugs, and that’s the be careful about keeping objects in reference that you don’t need because doing so can easily turn into a memory leak. A memory leak, in turn, is conceptually simple. It’s some path through your code that allocates memory and never frees it. Over time, as that path is traversed, your code will slowly allocate more and more memory until it, eventually, runs the JVM out of memory. The most common ways to do this in java is to allocate something inside of a function and then stick it into some form of static, or otherwise highly referenced data structure (cache is a good example). Glide has some built in defenses here (caches are built with WeakReferences for example) but it’s still possible to write a memory leak in Glide. So advice on that front is think *very* carefully before you put anything into a static collection.

So do I *like* garbage collected languages? The short version is I really do. It’s a lot easier to write code in java than c for example, and with today’s compilers and JVMs the performance delta between the two is quite small for all but a few exceptional use cases (like device drivers or extremely short spin up transient applications like grep). As an abstraction, java’s memory management and garbage collection approach works, and it makes the programmers job easier.

As a language feature, that’s about all I can really ask for,

--- Pat


## Pat Casey on code quality
I wanted to pivot back to technology to talk about something else that has been on my mind, and that’s how we measure code quality. In this context I’m not going to talk about the presence or absence of bugs in a piece of code or an algorithm; I’m not talking about that definition of quality. For purposes of this discussion, we’ll assume that all the code we are talking about does what it is supposed to and has no bugs (sic). Rather I want to focus on code quality in the more macro sense by looking at three different, often competing, variables we should be judging our code by and their relative importance: efficiency, expressiveness, and readability.

For our purposes, we’ll define efficient code as code that solves a problem with a minimum of machine resources, be they CPU, Memory, or Disk. In a lot of cases, those three vectors involve internal tradeoffs e.g. I can make code that uses no disk at all, but a truckload of memory, or I can write code that buffers lots of stuff to disk and hence needs little memory to run. Depending on the constraints of your particular system, either one of them might be considered “efficient”. In general though, code which is optimized for efficiency also makes a macro tradeoff in that it is harder to read and potentially less expressive. Consider the trivial and contrived example.

                int x = 7;

                X = x * 2; // x is now 14 but we needed a multiplication cycle to do it

                X = x << 1; // x is now 28 via a bit shift which is usually more efficient on intel chips

The first operation using the multiplication approach is easier to read because it doesn’t force you to do binary math in your head, but it’s probably marginally slower. With modern compiler optimizations, its possible the compiler would, behind the scenes, rewrite the first option as the second, but that’s almost independent of our decision making process as an engineer. We get to make a choice as to which formulation we want to use.

The second vector is, of course, code expressiveness. In theory, expressive code is code which efficiently encapsulates the programmers logical intention, and hence is highly correlated with readability e.g. expressive code == readable code. In practice though, highly expressive languages like ruby or perl often generate some of the hardest to read code in the engineering universe. The reason, I think, is that when given a highly expressive language, many programmers use it to optimize the number of lexical tokens in their code and *concision* becomes the de-facto synonym for  expressive. Consider a contrived example in a not particularly expressive language (java):

                private Boolean startsWithInteger(String s) {

                                if (s == null || s.length() == 0)

                                                return false;

 

                                char c = s.charAt(0);

                                if (Character.isDigit(c))

                                                return true;

                                else

                                                return false;

                }

We can make this a bit more readable (in my opinion at least) with a more concise formulation (a win win).

                private Boolean startsWithInteger(String s) {

                                if (s == null || s.length() == 0)

                                                return false;

 

                                char c = s.charAt(0);

                                return Character.isDigit(c);

                }

We can take it a bit too far though and make a harder to read version that scans in one line

                private Boolean startsWithInteger(String s) {

                                return s == null  || s.length() == 0 ? false : Character.isDigit(s.charAt(0));

                }

This last version uses less lexical tokens (and less whitespace), but is harder for most people to read. In the name of expressiveness, I have traded readability e.g. I took things too far.

The third vector of code quality is, of course, readability. As we’ve already demonstrated, readability often operates as cross purposes with the other two measures of quality. Readable code is sometimes slower than highly optimized, efficient, code. Likewise, highly lexically efficient “expressive” code is often harder to read than more lexically verbose code. Readability thus tends to become the sort of cobblers child of the code quality triad. It’s the thing we focus only after we feel like we’ve achieved the other two success criterria.  I would argue though, that as a professional engineer, working in an environment with a shared code base, readability is, if not the only measure of code quality, certainly the most important measure. Good code is code that another human being, who didn’t write it, and doesn’t know you, can read and understand. If they can read it, and understand it, they can extend it, fix it, or, even, optimize it. With a very small set of exceptions, my perspective is that readability is the ultimate expression of code quality.

The conventional wisdom is that the biggest enemy of readability a desire for optimization, and an excessive focus on efficiency. This expresses itself in aphorisms like “premature optimization is the root of all evil”. My personal experience doesn’t quite align with this though. I certainly do see highly optimized, hard to read, code in the environment. Sometimes that optimization is necessary e.g. there are some super high volume code paths where doing stuff like removing the function calls from your code and inlining the subroutines is actually worth doing, but they’re rare. Most of the time though, the optimization isn’t necessary and just serves to make the code harder to read. Still, this isn’t the failure mode that I see most frequently.

Rather I see the desire to write highly “expressive” and hence lexically concise code working at cross purposes for readability. Somewhere deep in the bowels of the engineering psyche there lurks an impulse to show off our mastery of a language through using all the cool fancy features. When used in a healthy way, this impulse can be used to take the semantic overhead out of our code, leaving only the logically meaningful tokens behind and hence making it easier to scan. When use in an *unhealthy* way though, our code starts looking like entries in the obfuscated c contest: lexically dense, contextually poor, and semantically opache.

My advice to the engineers in the audience is to redouble our focus on readability as the ultimate measure of code quality. If another programmer, in another part of the world, can sit down at your code and understand it, that’s great code.

Readability über alles,

--- Pat


## Pat Casey on the persistence layer of the platform

For today’s topic I figured I’d talk more about one of the more mysterious layers of the platform, specifically persistence. For today’s topic I’m going to focus just on *retrieving* data rather than *writing* data. Ultimately a persistence layers has to do both of course, but fundamentally writing data is almost trivially easy compared to reading it, so I wanted to focus on that part. For framing, we store virtually all of our data in a relational database. In theory, we support Oracle, MSSQL, or MySQL. In practice though, like 99.9% of the time, we actually use MariaDB which is an open source fork of MySQL.

Conceptually getting data out of a database sounds almost trivially easy. If you are using a protocol like JDBC you send a SQL statement like “select * from people” to the database via a driver, and then it returns something called a result set. You can then iterate the result set to see those rows that matched your query. It looks like simplicity itself, but behind the scenes its more complicated because the database and driver vendors are jumping through hoops to try to prevent you running yourself out of memory. Remember that databases can get big, and are stored on disk, so they are arbitrarily large compared with main memory. A single table might have a couple of TB of data even in a mid-sized system like ServiceNow. So when you actually execute your query and get the resultset back, the database *does not* actually return all the rows that match your query (slight oversimplification). Instead it returns a small subset (usually like 20 or 50 of them). As you iterate the resultset and “exhaust” those rows it has already pulled, it discards the 20 it pulled initially and pulls another 50.

Result sets thus, are intrinsically streaming. If you think about it though, that means that for every open resultset we have on the client, the database has to have some concept of the rows matching that resultset back on the server. After all, if you call next(), the database has to be able to pull the next row for you. Sometimes this can be done with minimal resources on the back end (like if you are just iterating a table in order), but sometimes the database has had to do a lot of work like create an entire temporary table to hold your results just so you can iterate over them. Because of all this, you have to *close* a resultset when you are done with it so the database knows it can clean up all those back end resources it allocated to keep track of your result. Forgetting to close a resultset is the JDBC equivalent of a memory leak in C … hard to do in theory but easy to do in practice and just as destructive to the stability of your code.

Now in ServiceNow, we want people who are not professional programmers to be able to *successfully* write code on our platform that pulls data out of the database. Equally importantly we don’t want a simple mistake on anybody’s part to take down the entire database by running it out of connections via something like a jdbc resultset leak. So ServiceNow has two (arguably three) layers in between the person writing code and the database itself. At the top of the stack we have GlideRecord, which is our usual data retrieval primitive. Below that though there’s a whole other layer called the Table layer, and beneath that there’s the database layer. When all of these are working together, they give a couple of cool benefits A) you can work with huge amounts of data without running out of memory and B) you don’t have to close your GlideRecord.

We achieve that trick through the magic of multi-part retrieval. Let’s assume you issued a GlideRecord query for all users in the system. At the base of the persistence layer, you might logically assume the database would issue “select * from sys_user”. But that’s not what happens. Instead our database layer retrieves only the primary keys for the result set. We would issue instead “select sys_id from sys_user”. That’s not enough data to fill in your GlideRecord yet (because you need all the fields in the table to do that), but it is enough information for me to know which rows satisfy your query. It also has a side benefit that it’s a *much* easier query for the database to resolve so the database, in turn, has to do less work. So we get a result set back that points to a lot of sys_ids, but it still a result set.

What the persistence layer then does is iterate over your result set and pull them all back up to Glide. In early versions of the platform we would just stick all those sys_ids in memory on the theory that sys_ids were small and hence any practical query would “fit it memory”. As assumptions go, it wasn’t bad, but it turned out to be wrong and customers (and our employees) did, in fact, generate queries that returned enough sys_ids to run us out of memory. So these days all those sys_ids (warning slight oversimplification) are broken down into chunks of 10,000 sys_ids and stored on the disk of the application server. If your query returns less than 10,000 sys_ids they sit in memory, but if its bigger than that we only keep one “chunk” at a time in memory and the rest of the chunks are stored as little gzipped files in the working directory of the app server.

We still haven’t gotten you any actual data though, all we have is a big list of sys_ids. So when you start iterating via something like GlideRecord.next() we have to actually get the *real* data from the database. What we do at this point is look at our list of sys_ids that we have in memory, pick the first 100, and ask the database to give us to full rows for those 100 sys_ids via a query that looks something like “select * from sys_user where sys_id in <a_list_of_100>”.  Databases are really good at selecting records by primary key, so these queries are very fast/low cost. One side effect though is that the rows don’t necessarily come back in the same order as the sys_ids we asked for, so when those 100 rows do come back there is a matching process whereby they get arranged in the same order as the original 100 sys_ids we started with.

GlideRecord, in turn, reads the first of these 100 full rows, sets up its GlideElements properly, and is now ready for you to do something like read a value, change a value, update something, etc. It’s pretty obvious that if you call next() a second time, we just move you to the 2nd of the 100 full rows we just retrieved, but what happens when you call next() 101 times and we run out of full rows? The short version is we throw away the 100 full rows we just collected and keep only the original 100 sys_ids. We then turn around and get the *next* group of 100 full rows (rows 100-199) and continue iterating. In turn, if you call next 10001 times, we throw away the current chunk of sys_ids (since its already on disk), and load the next chunk off disk, then load the first 100 full rows of the 2nd chunk.

This is all done transparently to you as an end user of course. From your perspective you are just seamlessly iterating over a large dataset via GlideRecord. Behind the scenes though it’s a complex dance of chunks of data flowing into and out of main memory. When it all works properly (which it typically does these days) it allows you as a developer to work with arbitrarily large data with a very simple, nearly foolproof API. You don’t have to worry about closing your GlideRecords, you don’t have to worry about returning “too much” data and running the instance out of memory, and you don’t have to worry about leaking resultsets and taking down the whole database.

As persistence layers go, in my opinion at least, it’s a pretty good one for our expected use case. I could write a faster layer that used lower level primitives and didn’t include the intermediate table layer, but doing so would put a lot more work on the people calling GlideRecord. For most of the stuff we write, “foolproof” or at least “hard to break” is much more important than “utterly optimal”. There are exceptions of course (the code that managed the chunks for example is highly, highly, optimized), but by and large hard to break abstractions like GlideRecord are what makes a system like this useful in the first place.

--- Pat


## Pat Casey on file systems

I thought today we’d veer back into a technical topic and I’d talk about something really in the weeds, but interesting from a technology standpoint. Specifically, I’m going to talk about file systems and storage. At a high level, file systems seem like one of the least complicated parts of a modern operating system stack, and potentially one of the easiest to treat as a black box. After all, you read files, and you write files. So long as the file system returns to same bits to you that you gave it originally, its done its job. How hard can it be?

The short answer of course, is that it can get very complicated indeed, and it often is, largely because of the archaic architecture roots of modern file systems. Most file systems in use today you see were designed in an era of spinning magnetic disks. These, in turn, worked by spinning a magnetic platter at high speed (4500 or 7200 are typical in consumer drives). To actually read and write data on the disk you have a little magnetic head that can move in or out relative to the center of the disk and either set the magnetic field of whatever piece of material is right below it, or read the magnetic field below it. To write out the first character of hello world, which is ASCII 72, you’d write out, in binary 1001000.

Now if you think about it, I can’t write 1001000 all at once or we’ll just rewrite the exact same spot of media underneath our head over and over again. That’s where the rotation of the disk comes in. To write that sequence we time our writes such that each bit is written on a slightly  different piece of media, leaving the particular sequence as a small arc across the disk’s media. If you remember your basic geometry it’s pretty evident that the outside edge of our circular platter is bigger than the inside edge, and hence if I write something on the outermost track with the same delay between bits, the individual bits will be physically further apart from each other (although they will be geometrically the same number of degrees apart). On older hard drives, you kind of ignored this fact and just wrote everything at a constant velocity. In more modern drives, bits are written faster on the outer tracks so they are physically about the same distance apart. This lets us pack more bits on the same amount of physical media, but it does make the drive more complicated to design.

Logically, reading, or writing, a piece of information on a disk comes down to two things. I have to move the read head to the appropriate distance from center such that, if I were to start writing, I’d be writing in the appropriate arc on the underlying media. Then, I have to *wait* for the underlying media to rotate such that the particular information I want is directly beneath the head, at which point I’ll start reading, or start writing.  In practice, you don’t read and write single bits off of a Winchester (spinning magnetic) drive. The way the drive controllers operate, you generally read and write 4k chunks off the platter. So if you need to store a tiny amount of information, it may all fit in one chunk and involve a single read/write operation. Larger files need to get broken up into lots of smaller 4k chunks though in order to be stored.

In modern drives, moving that read head is almost arbitrarily fast, so we can ignore any delay there. On the other hand, you can do some basic math and demonstrate that a 7200 RPM drive spins its entire plate 120 times a second. On average we have to wait for half a rotation before our piece of information is under the read head, so we can count on getting about 240 distinct “reads” per second, which works out to about 4ms per read.

In a sense, the math above is a bit misleading, because it assume *random* reads where I am moving the head at random all over the disk to read random stuff. If you imagine that I could sequence the reads in a row so that I was only moving the head once, and then reading over and over again as the disk rotated beneath the seeker head, you could imagine that I could read a truckload more data over time, and your imagination would be spot on. Almost all modern file systems are designed to lay a file into the smallest number of contiguous 4k sequences possible such that reading that file minimizes the number of disk seeks. There are even utilities you can run that will “pack” your hard drive by moving all the fragments of files around to try to glom them all together.

While that approach works ok for files you want to read once into memory, like a word document or a spreadsheet, it breaks down heavily for things like databases where the data is far too large to fit in memory and is accessed in user defined, quasi random, sequences. Nobody can predict ahead of time the exact queries you are going to want to run, so it’s extremely unlikely that the bits the database needs to resolve your query will be sequentially next to each other in one big streaming read. So database vendors, in general, wrote their own specialized file systems and bypassed the underlying operating system file system. The traditional way to do this was just to store the whole database as one big file and then manage the logically different things like tables and indexes yourself.

Technology marches on, however, and these days Winchester disks (spinning magnetic) are not typically used for primary storage outside of low end home computers. Most modern stuff uses solid state storage, colloquially known as SSD drives. These drives, in turn, have very different characteristics from the older Winchester drives. You can read data from any point on an SSD without having to move a read head or wait for a platter rotation so random reads are no problem at all on an SSD; any read is temporally equivalent, and fast. That, in turn, makes SSD’s both faster, and easier to manage, than Winchester disks. All the tricks file systems use to lay things down sequentially don’t matter in the context of an SSD.

SSD’s though, *do* have a problem, and that’s with writes. In common with Winchester drives, SSD’s write in 4k chunks. And, as you might expect, they can write anywhere in constant time so when you buy a new drive and start writing things to it, you see blazing speed. The worm in the apply though is that SSD’s cannot *change* data. They can only read, write, and delete. So when you modify a file, the SSD can’t change the data it has stored. Instead it marks the blocks that belonged to the old version of the file as “obsolete” and writes out an entirely new sequence of 4k blocks. This works great, until you run out of physical space on your hard drive. If most of the physical media on the drive is full of “obsolete” blocks, and you want to write a new file down, there’s no space. So to reclaim obsolete blocks, SSD’s have to erase it, and they typically erase in 64k or 256k chunks rather than 4k blocks.

The net result of this is that, once an SSD has been in use for a while, all the blocks either have files on them or are obsolete. And hence when you want to write a new file, the drive has to clean out a number of 256k chunks in order to make room for your new file. This is a problem for a couple of reasons. First is a pretty strong write amplification, in that you just wanted to store 4k of data, but behind the scenes the SSD had to copy like 6 4k blocks out of a 256k cell to make it empty, then erase that 256k cell, then write your 4k block.

The second problem though is the SSD storage cells don’t last forever. If you read and write the same cell enough times, it will wedge which basically means it’ll switch into a read only mode and can never be rewritten. From a drive’s perspective that means that particular cell is now useless and can never be used. So in practice SSD’s tend to pack in some extra cells above and beyond their listed capacity, and use the extra cells once some of the other ones start to die off. It also means that drive controllers will go out of their way to spread the workload around the media as much as possible so that specific cells don’t get over-written over and over again. If you modify the same 4k block over and over again in your application, a good SSD controller will be putting it on a different NVME cell every time you change it to avoid overloading any one block.

In this context, its best to think of a file system as a software abstraction that tries to hide how complicated the underlying media is to you, the end user. At an API layer you can just read and write files, and the file system will do the appropriate things in the background to try to optimize operations for the underlying media. It’ll try to lay the file down contiguously, it’ll delay writing things (or reading things) for a few milliseconds to see if it can batch operations together to make a more efficient read. It will, in general, try to do its best to perform well for you.

Where file systems can become problematic though is when their design assumptions don’t match the media they are sitting atop of. The most common issue being running a traditional file system on top of SSD media. A lot of the optimizations that a file system would do on a Winchester drive are, in fact, counter-productive or harmful on an SSD. On a Winchester drive we want to periodically re-pack the blocks on disk so they are as contiguous as possible for example. On an SSD, that has no performance benefit and degrades the life of the drive. More subtly, on a Winchester drive when you ask to read a file, there is usually a delay of up to 5ms before the file system even starts looking for your file since the os/file system is programmed to wait on the off chance that you or another process on the same host might as for *other* data that’s near the first in which case they file system will issue a batch operation rather than two single operations.

In the early days of SSD media, this impedance mismatch between the assumptions underlying the file system design and the characteristics of the new SSD media was a chronic problem. Windows Vista would constantly re-pack NTFS file systems for no reason, degrading SSD lifespans. Linux EXT3 would delay all reads and writes to an SSD by 5ms in order to pointlessly optimize the drive elevator, etc. These days those kinds of obvious errata have been resolved e.g. we run our a file system called XFS in our datacenters (its good at deletes), and we disable the drive elevator entirely so the file system doesn’t out-smart itself, etc. Net net of it is that the technology world has largely found a way to accommodate new storage media within the framework of our existing file systems.

From a meta standpoint, I don’t have any great philosophical point to make. The closest I could come would be to say that treating any part of a technology stack as a black box is a useful simplifying assumption. It makes understanding complex systems radically *easier* if we can just assume that file storage and retrieval is magical. We’ll ultimately write better code, and design better systems though, if we understand how those black boxes work.

So my advice on the technology front, and I suppose in life in general, is always seek to know *why* something happens or *how* something works and resist the temptation to float along on surface understanding.

The more deeply you understand a system, the better you will navigate within it.

--- Pat

And for those interested on how SSDs really work more in detail have a look at this interesting blog:

http://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/

## Pat Casey on text search
Today I want to talk about the history of the text search subsystem inside of Glide. To really do that properly though, I’m going to have to spend a chunk of time up from talking about how text search works conceptually before I start talking through the specific history of how we built the text search system in glide and some of the specific (and potentially non obvious) challenges we have to deal with. I’ll start by making a slightly trollish statement that text search really isn’t that hard of a problem if all you want to do is to provide *adequate* text search. The complexity comes in when you want/need to make *really good* text search that consistently gets people what they want when they do a search.

In concept though, like I said, text search is pretty easy. If I have a corpus of knowledge base articles about farming, and you type in “llama” as your text terms, I need to find all those documents which have the word llama in them. Those with lots of instances of the word llama in them, or the word llama in the title, should probably show up higher in the results than those with a single instance of the word buried in the text of the article, but we can separate the scoring problem from the retrieval problem for now and just say that we want those documents with the word llama in them. We’ll be making a few oversimplifications here, but by and large this is how text search words (it’s not magical).

Now a naive approach would be to just have the computer read every document in real time and only return those with llama in the text. Over even a medium sized corpus of documents though this gets impractical (it takes too long), so instead what we do is create an index. The key to our index is the word llama, and the index in turn points to each of those documents with the word llama within it. For whatever reason, these kinds of “word points to document” indexes are typically called inverted indexes, but there’s really nothing different about them than a normal database index like we talked about previously.

It turns out there are a couple of trivial optimizations we can make here though. The first is that we shouldn’t store the word “llama” or, any of the words in our index for that matter. Words are really big, use a lot of storage, and require multiple CPU cycles to do even trivial comparisons like equality checks. Its far better to convert the words into numbers, so there’s really two indexes at work. One associates the word llama with number 53, and the other index says that number 53 occurs in this or that knowledge article. The second optimization is that, for scoring, we want to know not just that the word llama appeared in the document, but how frequently, and where in the document it appeared. So the index will store not just the word -> document but also the position(s) within that document where the word appeared.

In the early days of glide, we knew we needed this kind of inverted index, and we did what pretty much everybody does when they first try to tackle a problem like this, we used Apache Lucene. Lucene, in turn, is an open source indexer and retrieval system where you feed it a list of documents, it generates and index, and then you can query the index for words of phrases and it’ll give you back the results. For glide, we also needed to write an *indexing* system whereby, as documents were changed inside of glide, we updated the Lucene index, but we’ll wave our hand over this for the moment. As an index and scoring system, it’s really quite good, but it from our perspective it had some challenges. Some of them we solved, and one of them proved unsolvable, but more on that later.

The first challenge with Lucene is that Lucene likes to store its index on the file system as a set of segment files. If you’re writing code that’s going to run on one JVM, that’s pretty convenient because you just embed the Lucene jars in your project, give it a working directory, and it’ll happily operate off of the local file system where you ran your app. If you’re running over a cluster though this is a challenge as it mean we need some central place to put the Lucene indexes. Each cluster node can’t do its own indexing or each JVM will have its own subset of the index and hence when you do a query against JVM1 you’ll get different documents back than against JVM2.

The first attempt to solve this problem was, in fact, to have every JVM keep its own index on its own file system. The indexer, in turn, was configured to send cluster messages around to every JVM every time it noticed something worth indexing, and then each JVM, in turn, would process that message and add the relevant information to its own local index. This worked much better in theory than in practice though since every time a JVM crashed, or we added a JVM to the cluster, we had to rebuild the entire index for that JVM which took a long time. Plus as our customers got bigger and their document corpuses got bigger, the cluster synchronizer was drowning under text index traffic. Something needed to be done.

So round two of the solution was to hack Lucene up a bit and delete all the stuff that wanted to store the index on a file system. Instead we implemented a *virtual* file system on top of our database, and then told Lucene to read and write its files there. If I remember right we had a table broken up into 4k blocks, like a file system, and indexed by offset so if you wanted to read 8k from offset 57, we’d do a select for offset 57+offset 58. Since there was not a central index file accessible to all the JVMs we no longer had to have each JVM do its own indexing, which cut down on the traffic in the instance. It did perform more slowly than the previous pure file system approach, but that ultimately wasn’t what killed this approach. Rather what did in the whole Lucene approach was hybrid queries.

The second problem with Lucene in Glide was that, perversely, we almost never do text queries. Rather we tend to do lots of hybrid queries which include both relational terms and text search terms. So instead of logically saying “give me all the knowledge base articles about angry llamas” glide applications almost always asked for something like “give me all the knowledge base articles where state=”open” and roles=”itil” that are about angry llamas”. Glide positively encouraged people to write hybrid queries like this, and both we and our customers did. Equally importantly, we couldn’t predict ahead of time what you’d want to query about since people could, and did, change their apps.

In principal, hybrid queries are no big deal. You just solve the two parts of the query independently, and then intersect them. In practice though, that goes very hard, very fast, as your document sets get larger. Consider that you want tasks where active=true and keyword=llama, and imagine we have a few million tasks in the system of which maybe 100,000 are active. I could start by first finding the active tasks from mariadb (maybe 100,000 of them) and then ask Lucene “search for llamas where key in <set of 100,000>”. Lucene performs incredibly poorly on this sort of query though, and it turns out that’s not really the optimal sequence in any event. A better approach is to first query Lucene for all those documents with “llama” in them, get back a smaller list (maybe 500 of them since llama is a pretty rare word), and then turn around and ask the relational database for those documents where active=true and sys_id in <list of 500>.

Since, most of the time, the list of keys coming back from Lucene is smaller than the list of keys that would have come back from the database, Glide consistently would solve the text query first, and then turn around and small that resulting keylist into a really big database query before appending all the relational terms and feeding that back into the relational database. As an approach, it worked most of the time, but there were degenerate cases where either the customer had a really big document corpus, or where the customer picked a really common word like “problem” and got a huge list of keys back from Lucene. These huge lists, in turn, would cause the relational database to choke trying to do something like sys_id in <500,000> sys_ids. At some point as well, the SQL statements themselves got too big for the JDBC driver to send over the max_packet size of the driver, and the database itself would fail to parse them.

We were close though, so we made any number of attempts to work around this problem. The first and most obvious was to try to push some of the relational terms down into Lucene. Lucene does support basic Boolean queries, and it does allow you to index scalar fields along with your text index, so, in our example above, we could index the active flag in Lucene, so we could resolve both halves of the hybrid query out of Lucene and declare victory. As an approach, this worked well on paper, but shockingly poorly in the field. For one thing, every customer was different so we couldn’t ever get the “right” list of fields to push into Lucene. For another thing, most glide queries involved joins, which we couldn’t resolve inside of Lucene. Finally, almost all glide queries are ordered by something other than text search relevance and it turns out that, while Lucene technically can order by a scalar field, it’s pretty slow at it.

By that point we’d sort of reached the end of our tether with Lucene; it just didn’t work for large document corpuses and hybrid queries. So we decided we needed to write our out text retrieval system, which ended up being called zing. Now conceptually zing is similar to Lucene in that it’s an inverted index where word -> number number-> document=positions. The key thing about zing through is that the index is stored as normal relational rows and columns instead of a virtual file system in 4k blocks of binary bits. So a zing index has an actual column called “word” and an actual column called “document”. Conceptually (warning, oversimplification) doing a query against a zing index involves *joining* the zing index with the primary data table something along the lines of:

Select task.sys_id, ts_index0.positions from task inner join ts_index0 where task.active=true and ts_index0.word=53; /* word 53 == llama */

There’s more complexity behind the scenes of course; zing indexes are broken down into 10 shards per primary table so we actually issue 10 queries, one against each shard, and then merge the results. Likewise there is a mess of code involved in doing stuff like converting words into numbers, and document keys into numbers, and packing all those positions into the smallest possible number of bits. Likewise scoring is itself a dark art/black magic and the code that scores the documents relative to your search terms is itself complicated and tricky. All of that being said though, zing, like text search, isn’t rocket science and for bog standard text searches it works pretty well, and, unlike Lucene, it works well for hybrid queries as well.

What zing doesn’t do though, is provide the kind of “modern” search experience today’s application users expect. It is, at its core, a text search engine, it’s not a full on information retrieval subsystem. Go to google and type in “what is six times nine” and google responds with a calculator and an answer (hint, its 54). Go to zing and zing responds with a list of documents containing the words “what six times nine” (is is a stop word and gets culled). The UI, in turn, just gives you have a list of documents with those keywords in it rather than trying to draw anything fancy like a calculator. What our customers (and our developers) are telling us though is that they want a more full featured information retrieval subsystem in the platform.

Now in theory, we could have built this as a layer on top of zing; somewhere in the bowels of google there is, in fact, a very large inverted index of the entire internet and that’s the raw underlying information that lets google do all the searches variations they expose. So it’s at least theoretically possible to build up that next layer of information on top of what we have, but that would take a long time and, frankly, it’s not our area of expertise. So we ended up buying a company called Attivio which as a more full featured information retrieval engine and framework. It has little genius boxes for example, that suggest results based on the perceived intent of your query for example.

As a technology, Attivio is based on old-school file system based Lucene indexes, and then has a service layer on top of it to do things like add the genius results and AI/ML components. We’ll be deploying it on a “one instance of Attivio per customer” model in our datacenters, and the various instances will talk to it via HTTP. If you’re thinking that we’re re-introducing some of the early problems we discussed earlier on, that’s only halfway true. Because Attivio is a centralized service that each JVM talks to, we don’t have the parallel index problem and, likewise, it has lower complexity than trying to virtualize a file system on top of a relational database. It does, however, still have the hybrid query problem across large corpuses though.

Our plan is to use Attivio specifically for what I’ll call the small/medium corpus searches in the platform where people reasonably expect enriched results. Portal search, knowledge base search, catalog search, etc. These have the advantage of being small enough that the hybrid query problem isn’t a big deal, and we can just query Attivio first just like we used to do for Lucene. They have a similar plus in that they are use cases that benefit a *lot* for enriched results, *and* have bespoke, custom, UI’s to display the results so we can do the UI work to render the enriched results as well. For large corpus results though, most especially including keyword queries as parts of reports or lists, we are continuing to use zing since it’s the only technology we have that’s performant for those sorts of bespoke, hybrid, queries.

I don’t have a grand philosophical point to end with today, except to point out that we’ve grown to the point as a company and a technology stack that one-size-fits-all solutions to broad problems like text search are not necessarily the right approach. My engineering and product bias is still very strongly that a single unified approach is the strongly preferred option, but it’s not *always* the right option :).

--- Pat

## Pat Casey on compression and encryption

Today, we’re going to talk about compressing and encrypting data. We’ll touch peripherally on something called information theory, but I’m going to intentionally steer clear of the math here and just stick with the general concepts. My goal here isn’t to equip you to write a signal processing system, but rather to instill a broad understanding of how compression and encryption work. Along the way there may be some light references to math, but there will be no quiz (I promise).

I’ll start by talking about a form of information we’re all pretty familiar with, language. English, for example, has something like 175,000 words in it, which is a lot. Even if you drill down and look at just six letter words, you get something like 22,000 six letter words, which is still a lot. It’s really not that big though relative to the number of *possible* six letter words you can construct with the English language alphabet. Because English allows for repetitive letter use, you can re-use any letter at any point in the sequence and we can theoretically construct 26^6 or 308,915,776 possible unique six character sequences based on our 26 letter alphabet. From a standpoint of informational density, English isn’t particularly efficient.  Mathematically I could have expressed 22,000 unique constants with something between 3 and 4 letters. Using only three I’d have been able to express 17,576 concepts, while with 4 I could have expressed a whopping 456,976 concepts.

If you imagine that I wanted to send a message using the smallest possible number of characters, it might make sense for me to map each of those 22,000 six letter words onto shorter, 4 character sequences, and just send those shorter sequences. You, in turn, could then just reverse the process and get back the original words. Statistically I’m very, very, unlikely to use more than 17,576 distinct six character words in any message, so I could probably get away with mapping each of the *actual* words onto a 3 character sequence, but I’d need to use a different mapping for every message. If the messages are relatively long though, I could just append the mapping to the beginning of the message and still end up sending you less characters than I would have had to if I’d used the previously agreed upon 4 character encoding model.

Compression algorithms, at least the loss free ones, are really just fancy versions of what we described above. These are algorithms that scan through a sequence of bits and replace repetitive elements with shorter tokens. If your compression algorithm is perfect, you should only need to use as many tokens as there are unique sequences in the original dataset. To send a message you thus first construct the compression alphabet, which you send to me, and then you tokenize the message as it flows through, replacing each sequence with its new, shorter, token. Modern streaming algorithms are a little more fancy in that they create the tokens on the fly and embed them in the information stream as they go rather than pre-processing the whole message. Likewise, modern algorithms use variable length tokens and encodings, but the concepts are the same.

A compressed message is thus *denser* than an uncompressed message in that you can require less bits of physical storage to encode the same logical amount of information. You might intuit that highly repetitive messages with very few unique sequences within them are intrinsically more “compressible” than messages with lots of unique sequences within them, and your intuition would be very much correct. There is, as you might suspect, an entire branch of mathematics called Information Theory which deals with quantifying just how dense an encoded sequence of bits is, and equally importantly how many discrete datums of information are contained within it. The number of unique sequences (and their relative probabilities) ends up defining the smallest possible number of compressed tokens I can use to encode the information. Information theory thus tells us that there is, in fact, a lower bound of compression below which we cannot go for any given sequence of bits.

So what’s a computer scientist to do when asked to store really big sequences in really small numbers of bits? The short answer is, we cheat, and violate the lower bound by throwing away information that we don’t think you’ll notice. Things like JPEGs (for images) or MP3 (for audio) or MPEG (for video) all operate at efficiencies far above what normal compression can achieve through the simple trick of throwing away information in the process. A JPEG will take adjacent bits of am image that are “nearly” the same color and express them as “exactly” the same color and otherwise blur the sharp transitions in an image. An MP3 will model a trumpet note as a perfect gaussian curve, even if the player had an imperceptible warble in it. An MPEG will render sequential frames of an image as deltas relative to the previous frame, and, similarly to a JPEG, will flatten color or blur edges. To make this less noticeable to you, these formats use various tricks. JPEGs for example, encode in diagonal lines running from corner to corner on the image rather than in horizontal lines to better “hide” the color cramdown from your eyes.

There’s another, similar thing that comes down to information though, which is distinct from compression, and that’s *encryption*. Modern encryption has a variety of implementations, but one of the aspects of computational encryption is that it randomizes the bits in the cyphertext, and that has a side effect of rendering the cyphertext uncompressible since compression relies on the inherent redundancy of data in normal files. In truth this isn’t a side effect so much as a designed-in feature of good encryption, as letting any trace of the original information density of the message “leak” into the cyphertext gives a potential code breaker a huge advantage if they try to crack your encryption. In fact, one of the ways to test if you have properly *decrypted* a message is to calculate the entropy of the resulting plaintext. If you get a low value, you have probably successfully decrypted your message. This leads to the rule of thumb in signals processing that you always compress before you encrypt, because if you encrypt first you can’t compress.

At one point in my life I was pretty good at information theory in that I got paid to write pascal code that would calculate the entropy over DNA sequences (it’s a long story). What I didn’t have at that point was a firm conceptual understand of what the hell I was modeling though. I could (and did) write code that calculated entropy correctly, but I didn’t really understand what it was. As a refresher before I wrote this thing I looked up a couple of articles on information theory and found myself in the exact opposite state of my youth. At this point in my life I understand the *concepts* behind information theory pretty darn well, but I was getting lost in all the discrete math. Maybe I’ve been doing too much quadratic algebra with my daughter, but I couldn’t motivate myself to get out pen and paper and manually work the entropy equations a few times to get a renewed feel for them.

I’d say that for most people, most of the time, even for most computer scientists, understanding the concepts of information theory, or any kind of similarly complex theoretical framework, is far more important than knowing the implementation details. If you know the concepts, you can always look up the implementation somewhere online and you’ll have a mental framework to fit it into. In contrast, if you know the details, but not the concepts, you’ll be like me at 18, perfectly capable of calculating the entropy of a strand of DNA, but utterly unable to make the conceptual leap to understand how compression works.

My experience is that, if you’ve got the concepts, you can look up the details, but the reverse is rarely true,

--- Pat

 

## Pat Casey on one of his favorite classes in glide

One of the more interesting classes (to me at least) in ServiceNow is the rather unassuming piece of code called AbstractBucketCollector. In a nutshell, AbstractBucketCollector is the data structure that glide uses to track most of our in-app metrics about things like transaction response time, jvm memory in use, and a myriad of other health performance metrics. If you look at the stats.do page or the xmlstats.do page on a glide node, most of the interesting numbers you see there are stored in, and then extracted from AbstractBucketCollectors.

At a high level, the thing it wants to do sounds pretty simple. There is a metric like “response time” that we want to record every time we complete a transaction. We will then periodically ask questions like “what was the average response time in the last 5 minutes” or “what was the median response time in the last 15 minutes”. You could imagine a straightforward data structure where I just naively stored an array of tuples where timestamp -> response time e.g. 12:00:00 -> 00:00:02.36 would mean that at exactly noon a transaction happened with a response time of 2.36 seconds.

As a data structure though, that kind of approach is pretty inefficient. First of all I’m storing lots of timestamps that I really don’t need except directionally, and second I’m storing an indeterminate number of rows. On a busy node I could end up dedicating an awful lot of memory to tracking all of those transactions. Logically I don’t have to store the transactions *forever*, so discarding those which are more than 24 hours old would keep it from trending to infinity, but it could still get to be a very large and unpredictable number. On a lightly loaded node I wouldn’t need much memory, but on a busy node 24 hour’s worth of tuples would be a lot of storage. From a platform scale and management standpoint that wasn’t great. I wanted a data structure which didn’t use a lot of memory in the first place, and which used a predictable amount of memory when it did run.

Getting rid of the timestamps proved to be not too difficult. Since questions were always asked about the metrics in terms of minute ranges, I could store all of the response times for a given minute in a single array and then just tag the array with the minute boundary. That way I’d store one timestamp per array, rather than one timestamp per data point which saved a lot of space. I could then store an array of arrays (a matrix if you will) with each row of the array representing a particular minute. I also could (and did) make this rotating buffer fixed length, so by default I stored only 15 minutes worth of data, but for some use cases I extended the class to allow storage of up to 24 hour’s worth of metrics via the IDailyStats interface and its various implementing classes.

That got rid of the duplicative timestamps, but it didn’t solve the problem of needing a variable, and un-bounded number of points per minute. Each minute’s worth of response times would have a variable number of points in it based on the count of transactions that happened to take place in that window. So I made the arbitrary decision to store no more than 100 points per minute. Logically thought I knew I’d need to be able to answer questions like “what was the minimum, maximum, average, median, or 90% time” for a given one minute sample, and that was going to be a problem if I didn’t want to keep all of the data points.

Some second grade math will tell you that you can track minimum, maximum and average values of a series without storing the series. You just keep track of the biggest number you have seen in the series (max), the smallest (min), the total number of points you have seen (count) and the sum of all points thus far seen (sum). Average is then trivially computed by dividing sum by count. So you’d be tempted to think this was a trivial problem, and it would be *except* for medians and 90% points. The textbook definition of a median is that I sort all the values in a series and pick the midpoint. To do that, I need the whole frigging series, don’t I?

It turns out that, in fact, you do, but that you can get *pretty darn close* to the median of a large series through sampling. To calculate the median height in the united states I don’t really need to record and order 300m people’s height. Instead I can randomly select a few thousand people, and find the median of that subset of people. Similarly I can get a really good estimate for the median response time for a very large series of response times, even if I only save 100 data points, *provided* those 100 data points are randomly selected from the overall series.

That, in turn, is a bit of a subtle problem since I don’t know how big my series is while I’m recording. I just record every point that comes my way for 60 seconds. It might be only 10 points, in which case I can keep all of them, or it might be 1000 points, in which case I need to keep a randomly selected subset of 100. When the first point arrives though, I don’t know how big the series is going to be, and hence I don’t know whether or not I’m going to need that particular point when I’m done. Maybe the series is small, in which case I’ll want it. Maybe the series is big, in which case odds are pretty good I won’t randomly select that particular point as a keeper. But I can’t peer into the future to determine a storage strategy in the present.

It turns out though that there is an approach which works here, and it’s conceptually straightforward. What you do is you keep every point that comes your way until you have the maximum number of points (100). When the 101st point arrives, you do some basic math. If we knew the series had 101 points in it, the odds of this particular point making into the “keep” list is about 100/101. So we roll a virtual dice and see if the “new” point makes it onto the keep list. If it does make it onto the keep list, we randomly replace one of the existing data points with the new one. We just keep repeating this as the series increments. Point 1000 for example, has roughly a 100/1000 chance of being selected in a “fair” selection out of 1000 entries, so it has only a 10% chance of making it onto the keep list. One subtle effect on this that it’s easier to get onto the keep list if you are one of the early data points in the series (the first 100 always get on there), but it’s harder to *survive* on the keep list since you have multiple chances of being evicted as later elements in the series win their dice rolls and make it onto the list.

Somewhere, in my mathematical hindbrain, I’m concerned that there is a subtle error in my eviction logic and my algorithm doesn’t, in fact, give every point an exactly equal chance to make it through into the keep list, but empirically it’s a drop dead simple algorithm and it's close enough. Even though we’re only storing 100 data points, we are tracking enough information to give accurate answers to minimum, maximum, average, count, median, and 90% point. Which is great in that it gives us a memory bounded way to compute the median of a series of indeterminate length.

Once we have a data structure into which to store our metrics, we need a way to push metrics into the data structure. For some metrics, those which we measured on a regular basis (usually every second), I created a thread whose job it was to wake up every second and collect a mess of metrics, called, imaginatively, InternalMonitor. That thread, in turn, would poll a bunch of internal counters, and store the values into AbstractBucketCollector. It also served a double duty in that, every 60 seconds, it would tell the various collectors to roll to a new minute via the flush() command. You could have imagined that the AbstractBucketCollector could have figured that out on its own, but it was convenient to code it this way.

There are other metrics though, things which happen at unpredictable times, like transactions arriving, or completing. To collect *those* metrics I needed to push data into the metrics subsystem every time “noteworthy” things happened. For these things (like response times), I generated singletons like ResponseTimes whose job it was to store the response time metrics. I then stuck the singletons into the list of monitors the InternalMonitor thread was managing consistently with the measurement metrics. To actually poke metrics into it though, I had to drop an entry into our transaction pipeline and implemented a new TransactionMonitor called, of all things, ResponseTimeMonitor, and then our transaction pipeline naturally invoked notifyTransactionComplete on that class on the completion of every transaction.

As a metric collection system, there’s parts of it I like. The way we collect measured metrics via the InternalMonitor thread, and the way we store metrics over time via the AbstractBucketCollector are, I think, pretty well crafted and, dare I say it, comprehensible, code. I’m not really in love with the way I had to insert measurement code into lots of spots in the platform though in order to “poke” those types of on demand metrics back into the central repository. Given my druthers and an infinite amount of time I would prefer to have built a more generalized callback framework, but for the week or so I had to dedicate to this project, the current approach works even if it’s not all that elegant (imho at least).

As a subsystem, the metrics collection system isn’t the most sophisticated or complicated part of Glide, but it’s been chugging along doing its job successfully for ten years or so with very little need for care and feeding e.g. it’s a low maintenance subsystem. By my standards, that makes it a pretty successful project. I’d like to claim there’s a lesson here, probably about focusing and doing our best work even when we aren’t on a particularly high profile or exciting project, and that is, in fact, a really good lesson. The truth in this particular case is more prosaic though, in that I remember really enjoying myself as I wrote this code. Something about the intellectual challenge of cramming a lot of metrics into a small amount of space got my creative juices flowing.

Sometimes the little projects are far more satisfying than the big complicated ones,

--- Pat

## Pat Casey on caching in Glide

Today I’m going to be talking about how glide manages caching. There will be a little incidental coverage of some basic caching theory, but by and large this is going to be about how Glide caches stuff, why we cache the stuff we choose to cache, and how we manage things like cache invalidation. The goal here isn’t necessarily to equip you to write your own cache implementation, but rather to help you understand, and hence better utilize, the underlying cache infrastructure.

I’ll start by making the rather obvious point that computers cache stuff in order to reduce the amount of work they have to do. In a system like glide there are lots of data structures we generate that require a lot of work (cpu, database, or network) and time to construct. If those structures don’t change very often, it conceptually makes sense to just generate them once, stick the results in memory somewhere, and then use the results you have in memory instead of reconstructing the whole thing every time you need it. It’s evident that there’s only a benefit to this approach if pulling the data out of memory (the cache) requires less time than constructing the thing in the first place, but that’s usually a pretty easy bar to cross. It should also be pretty obvious that if the underlying data were to change, you need some way to discard the version you have in memory because it’s now out of date.

When I set out to write the cache manager in Glide, I had two different problems I was aware of at the time. One was, pretty obviously, that we needed to cache stuff, and we needed some easy way to manage API to handle caching. The second, more subtle problem though, was that not everything we wanted to cache had the same lifecycle or invalidation criteria. Some things like form templates are very, very expensive to generate and hence are great caching candidates, but they are accessed so infrequently compared to something like a TableDescriptor that they will inevitably get pushed out of any unified cache implementation by a spam of cheaper to generate, but more frequently accessed, items. So I was aware that I needed to design a cache system that handled different kinds of cachable content differently.

Hence was born CacheManager.java, which is the entry point for caching in Glide. Conceptually, CacheManager manages a number of *distinct* private caches, each with their own rules and invalidation cycle. Caches, in turn, are *usually* (more on this later) instances of SynchronizedLRUCache which, as the name implies, is a thread safe implementation of an LRU cache with a fixed size. When you create a new private cache via the CacheManager.addPrivateCachable() API you are actually telling the CacheManager to generate a shiny new cache for your use that will be kept distinct from every other cache in the system. You can also suggest the number of entries you want in the cache by providing a size hint when you invoke the API. That value is just a suggestion though, in that it can be overridden via a system property in the form glide.cache.size.<NAMEOFCACHE>.

Another thing you can do is tell the cache manager to take care of cache invalidation for you, and you do this by using the CacheManager.addPair(name, friend) API point. What addPair does is tell the cache manager that whenever the first named entity is invalidated, so too should the friend. *Most* of the time the first named entry is a table, and the second entry is the name of a cache. So what you are logically telling the CacheManager, for example, is “whenever the sys_plugins table changes, invalidate the plugin cache”. If you set up a cache pair like this, the caching layer will take care of invalidation for you whenever the underlying table changes. For historical reasons, the first named entry can actually be the name of another cache, and if you set things up that way when the first cache is flushed, so too is the second.

Most of the cache invalidation in the system is managed via this (relatively) straightforward mechanic. It works, it's handled at a super low level by the system, and it requires almost no work on the part of the implementer. It does have a failure mode though in that it is a very coarse invalidation mechanic. Any change, to any row, in the underlying table will invalidate the entire named cache. If you imagine something like the TableDescriptor cache which tracks the table structure of each of the five thousand or so tables in Glide, it seems intuitively obvious that throwing away all five thousand cache entries any time a single row in sys_dictionary changes is going to be inefficient. Logically if I change one row on sys_dictionary that describes the short_description field in task, I should only need to invalidate the tabledescriptor for task and any sub-tables like incident/problem/change.

To handle *that* kind of implementation, you can take control your own cache invalidation by making your own cache implementation which implements the ICacheProvider interface and then not registering any cache friends. In doing so you are telling the cache implementation to never clear your cache outside of a global cache flush triggered by something like a boot sequence or a plugin installation. You probably *do* want to clear your cache periodically when certain things change in the underlying database, so the system provides some helper utilities to track what’s going on, generally the DBAction.registerListerner() API point. That, in turn, will tell the database layer to inform your listener class whenever any insert/update/delete action occurs on *any* table in system. In practice, you probably only care about a changes to a few tables, so the first thing most implementations of AChangeListener do is exit early if the database table being changed isn’t interesting. For a complex, but pretty complete, example of how this is done, take a look at TableDescriptorCache and the way it interacts with the rest of the system.

Of course, glide is a cluster, and each node has its own block of memory allocated for cache, and an invalidation event might happen on only one node but we nonetheless need to invalidate the cache on all the other nodes. The CacheManager takes care of that for you by enqueuing cluster messages anytime it flushes either an entire cache, or a single cache entry. So long as invalidation calls the CacheManager.flush() API point, it will properly propagate across all nodes in the cluster. You’ll notice that even in the implementations of classes which handle their own invalidation, they still route the invalidation logic back out through CacheManager in order to piggyback on that built-in logic and propagation mechanism.

One of the more subtle points of the CacheManager though is that not every cache is actually an in-memory LRU cache. There are some specialized cache implementations for specific kinds of data. You can tell the CacheManager what implementing class to use for your particular cache by setting a property in the form glide.cache.provider.<NAMEOFCACHE>. For highly referenced singletons that nonetheless need invalidation logic, there is the SingletonCache implementation that allows for largely latch-free reads of an underlying singleton (although it does introduce an implicit memory barrier through the use of a volatile variable). For large chunks of data that can be serialized as a string or byte[], but don’t fit easily in memory, there is DiskCache which stores cache entries on the file system of the app server. For expensive to generate data that can be serialized as a string but doesn’t need to be accessed super frequently, there’s DBCache which puts cache entries as blobs in the database. For data with indeterminate size where caching a fixed number of entries runs the risk of burning too much memory, there’s SizeLimitedCompactCache which manages for a fixed memory footprint instead of entry count. The list goes on from there.

Given the flexibility of the cache manager, it’s pretty much inevitable that engineers will do unwise things with the caching layer so glide builds in *some* defenses against common cache failure modes. It doesn’t defend against all of them though, so the only real defense is care and diligence on the part of the implementing engineers (that would be us). With that being said, I’ll go through some of the common failure modes of caching and what we’ve done about them (or have not).

The first and most common failure mode of caching is caching *too much* stuff, generally to the point where the cache starts consuming so much system memory that the system either can’t do anything else, or flat out runs out of memory. To defend against this, the glide cache subsystem actually stores soft references to the individual caches. What a softreference does is tell the JVM that if it feels under memory pressure it can discard whatever object tree is being referenced. In practice that means that if you try to cram too much stuff into a cache and put the JVM under memory pressure, the garbage collector will come through and clear the softreference to your cache, essentially discarding it all. Not great from a cache efficiency standpoint, but far better than running the instance OOM. As documented, the JVM should only start clearing out soft references if it's nearly out of memory, but in practice the GC we use seems to clear them out more frequently than that, especially if it hasn’t traversed the reference very frequently.

The second more common failure mode of caching is to cache highly volatile stuff that is being invalidated extremely frequently, often more frequently than it is accessed. This has two obvious downsides. One obvious one is that you get almost no benefit from the cache because even though you are dutifully sticking things in the cache, they are being invalidated long before any other thread (or even your thread) ever gets around to reading them back out so you are essentially doing a lot of work for nothing. The more subtle, but more pernicious, failure mode is that all those cache invalidations send cluster messages around inside of glide, and you can gum up the cluster synchronizer if you spam it with enough cache invalidation messages.

The third common failure mode is to cache complex objects and not properly recognize the dependency graph for invalidation purposes. You might cache a form template in memory for example, and properly remember to invalidate it when the sys_form table changes, but not remember to invalidate it when the sys_form_section table changes. If you make errors like this you end up with logic errors in the system where you’ll change something but the system won’t reflect your changes until some random point in the future where the cached version is cleared out for some other reason. These tend to be the most pernicious and hardest to debug caching errors so they’re the ones you need to pay a lot of attention to.

The fourth common failure mode is even more obvious and it’s to cache stuff you don’t actually need to cache, either because it’s so cheap to generate in the first place, or because you almost never reference it. In a sense this is the most benign failure mode since having a few “extra” and rarely used things floating around in the cache rarely brings the system to its knees or otherwise breaks anything. Still, taken to its extremity, this problem will eventually fill up the available cache space with not very useful stuff. The only way I know to defense against these is to be constantly checking the behavior of caches in the real (production) world. If we find stuff that’s being cached that is virtually never being referenced, it’s likely a good idea to stop caching that particular tranche of information.

The reality is that the glide platform *already* caches a whole mess of stuff on your behalf, so it’s rare that there’s actually a need for an individual engineering team to add a new set of cacheable objects. If you find yourself wanting to do so, that’s a great opportunity to meet your friendly neighborhood architect and make sure what you are proposing is actually a good idea. That doesn’t mean it isn’t a good idea mind you; we add new cacheable stuff frequently, but it’s something of a high bar. Generally speaking, caching metadata like form templates, flows, assignment rules, and whatnot tends to make a lot of sense in glide since they rarely change and are frequently used. In contrast caching primary data like incident/problem/change almost never makes sense because they change frequently and are trivially easy to read from the database when needed.

It’s worth pointing out that you can actually get some pretty good debug metrics out of the cache via the xmlstats.do API point. If you are logged in as an admin, or connecting from inside of the ServiceNow datacenter IP space, you can pull cache metrics out of an instance with a URL that looks like: Hi.service-now.com/xmlstats.do?include=cache You’ll get back a block of xml describing the cache health of the *particular* node you connected to (remember each node has its own cache). For example I pulled the data out of one of the HI nodes as I was writing this. The particular node I connected to had been restarted just before 11:00 AM this morning so it’s been up and running for about three and a half hours.

Here’s an example of a very frequently accessed cache: <syscache_tabledescriptor entries="7815" flushes="0" hits="229885789" max_entries="2147483647" misses="25261" puts="8023" queries="229912304" reclaims="0"> What this is telling us is that in the 3.5 hours since this node was started, we have put 8023 things in the TableDescriptor cache, of which 7815 are unique and are still there. Also in that time we have queried the TableDescriptor cache 229,912,304 times, or roughly 65,681,654 and hour, or 18,244 times per second. It is a pretty busy cache.

In contrast we can look at:<ui_notification_content_cache entries="1" flushes="0" hits="18" max_entries="1000" misses="6" puts="3" queries="24" reclaims="2"/> What this is telling us is that since the system came up, we’ve put a total of 3 things in the cache, and as of right now there is one thing in it. In that time we’ve queried it something like 7 times an hour, and the JVM has made the decision to throw it away to conserve memory twice already. All in all this one looks like a candidate for investigation e.g. are we really getting a benefit out of this cache? Maybe its super useful in some contexts, just not the way HI runs, but then again maybe it’s just not that useful a thing to cache at all.

Anyway, the point of all this isn’t to encourage every engineer in the company to start caching more stuff. If anything my instinct is exactly the opposite in that I find it much more common for engineers to *over* rather than under-cache things so I don’t want folks to view this as an instruction manual on caching more stuff. I do want folks though to understand how the caching subsystem works both to help generally understand and debug the system, but also to make better decisions in those rare cases where they do, in fact, have something worth caching.

--- Pat





## Pat Casey on the legacy rendering layer: a cautionary tale

There are various things in the Glide platform I’m proud of. I think the database layers is an excellent simplifying abstraction over the underlying relational model. Similarly, I think the scheduler and cluster implementation is a piece of excellent engineering judo in that we’ve managed to build a cluster that handles all inter-node communication via the database. The list, in fact, goes on from there. There are, as I said, lots of things in glide I’m proud of. Our Jelly based UI rendering layer, however, isn’t one of them. In my opinion it violates any number of rules of good engineering design or, for that matter sanity. So for today’s discussion we’re going to look at how we got here as something of a cautionary tale. Note that I’m specifically talking about the engineering implantation that puts together the HTML that your browser renders, not making a commentary about how that HTML looks and feels once your browser actually renders it.

In the beginning, Jelly wasn’t the abomination it was today. We used the apache library more or less as is and added our own tags. Rendering was done in one pass, and the underlying templates were pretty comprehensible by a normal human. Jelly did have a couple of intrinsic flaws that made it harder to use that we had to deal with but they weren’t crippling. The main flaw that’s worth pointing out though was that Jelly didn’t use explicit variables, you just kind of declared a named token called like jvar_badger in one piece of code and then like six months later in some other unrelated piece of code you referenced the value of jvar_badger. If you were the guy who created jvar_badger in the first place this sort of worked, but in a team environment, if you were not the guy who created jvar_badger it was really hard to even know that it existed, or what it had in it, or how it got constructed in the first place.

Still, if that was the only thing wrong with Jelly, I wouldn’t be writing this essay. What happened over time is that we started adding our own set of mistakes to Jelly that made it harder, and harder, and harder to use. The first mistake was very well intentioned and based on a premise that *seemed* to make perfect sense. We decided that building forms wasn’t really a single thing. Logically, we should first build the “structure” of a form, and then we should subsequently bind the data into it. Conceptually this is totally reasonable, but we decided to implement it in Jelly through a really weird approach of rendering all jelly templates twice. The first time through we’d process all the <j:> tags, and the second time through we’d process all the <j2:> tags.

In theory this makes the pages simpler to read, since you know that <j:> is structure and <j2:> is data binding. In practice though, the intellectual streams get crossed and we ended up with structure in <j2> and data in <j>. To add insult to injury the way we embedded one template in another meant that if you embedded template B inside of template A, and called template B via a  <j2> tag, then all the tags in template B only saw one rendering phase and hence only their <j:> tags ran. Sometimes, in short, <j:> tags actually ran in phase 2. It added an incredible amount of complexity to the system and made a templating language which was already hard to read even harder to read and mentally process.

It gets worse from there though because we exacerbated the problem by mixing a whole other language into Jelly. You see Jelly had its own, tag based, scripting language. I’m actually not positive if it was technically Turing complete (it probably was), but it let you do stuff like loop, compare, iterate, set, get, etc. Basically all the variable manipulation stuff you’d need to render your pages. The whole rest of our product though was based on using javascript as a scripting language, rather than Jelly, so we decided Jelly would be easier to use if we added the ability to execute *javascript* inside of the Jelly context. Hence was born the infamous <g:evaluate> tag. That tag, in turn, let us access our full rhino scripting environment inside of a Jelly script and then poke the return value of the function into a Jelly variable.

So now we have a language with mysterious, “magic word” based variables, a complicated and inconsistent two phase rendering approach, and two distinct execution languages which didn’t *quite* match up properly. But it just keeps getting better. In the early days of the glide javascript engine, we pretty much exposed GlideRecord and GlideSystem as the two root objects. If you wanted to do something complicated that wasn’t already available via a GlideSystem API call or trivially implemented with a couple of GlideRecord calls, you had to write a new GlideSystem API point. Need to get the difference between two dates? Go ahead and write gs.dateDiff();

As an approach that worked, and had the added benefit that we knew *exactly* what was exposed in the scripting world. GlideRecord and GlideSystem were our public API, and everything else was private. It was inconvenient though, in that if you were in the middle of writing a jelly script and needed to do something complicated you ended up banging out some script inside of a g:evaluate tag, and then having to switch into java to write a new GlideSystem call. At which point one of the early engineers, discovered the abomination that is Rhino’s Packages syntax. Once you turning it on the Packages syntax exposed literally the entire JVM into the rhino context. Suddenly you could do var dbq = Packages.com.glide.db.DBQuery(‘sys_dictionary’); via some trivial use of reflections, you could even get ahold of private classes, private methods, and private variables.

There was, to be fair, a bit of an internal debate as to the wisdom of using Packages calls. You can probably guess from my use of the word “abomination” to describe the calls in question which side of the debate I was on, but in the end the siren call of convenience won out and we started using Packages calls extensively. They were, in fact, exposed not just inside of Jelly rendering, but anyplace in the platform where rhino could be invoked. It did let us write code faster, but had three noteworthy side effects. One was that we couldn’t refactor anything without taking the risk of breaking some mysterious Packages call in some random piece of code. The second was that it was impossible to secure the platform if any method on any object could be invoked from script. The third, more subtle problem, is that we lost control of our API and hence didn’t know the surface area to test.

We eventually, and laboriously, managed to climb out of the Packages call hole. These days glide doesn’t let you do that outside of a well understood list of classes that we have explicitly (and consciously) exposed into the rhino context. The rest of the “quirks” in our Jelly implementation remain in place even today. At least part of the reason we are building an entirely new UI layer is to change out the underlying rendering technology and get rid of Jelly.

It's worth taking a step back through and looking about how we managed to dig ourselves into the hole in the first place. I’d say there were really two conceptual mistakes we made. The first mistake was to stick with a bad architectural decision (two phase rendering) long after it became clear it was adding incredible amounts of complexity to Jelly for limited benefit. We could have switched it all back to one phase rendering in a couple of weeks back in 2010 and been far better off for having done so. The second two mistakes (the g:evaluate tags and the Packages calls) were really based on solving for *convenience* rather than *correctness*.

I’m not going to make the argument that one should always solve for perfect correctness rather a convenient hack. Sometimes, the three line hack really is a better return on engineering investment than re-factoring a whole subsystem. In this particular case though, we were building the UI rendering subsystem for what, even then in like 2010 was pretty clearly growing into a successful product line. We should have bit our tongues, kept rhino out of Jelly, and kept the Packages calls out of Rhino. If we’d done that, we’d have a much more useable, though probably still Jelly based, rendering system today.

Mistakes were made,

--- Pat


## Pat Casey on expression caching and how we made it work in Glide

The other day I spent some time talking about caching in Glide, and I got a couple of followup questions that were actually about expression caching, which is something of an extreme special case so I thought I’d spend some time here today talking about how Java does dynamic code compilation in general, and how it impacts glide in particular. We’ll spend a bit of time on theory here, but by and large we’ll be talking about the Glide implementation rather than doing a whole mess of JVM internals. Still, it’s important to understand the underlying mechanism so we will start there

It’s probably not news to anybody that if you write some java code on your laptop and then execute it, the source code on your laptop gets compiled into some form of byte code representation. The way Java works, your source code gets compiled into a Java .class file which, in turn, represents a series of instructions in the JVM’s own internal primitive instruction set. When you run your code, a modern hotspot JVM will actually compile that Java byte code a second time into instructions specific for the CPU its running on, but that’s largely outside the scope of today’s discussion. The key point is that source code gets compiled into byte code, and byte code, in turn, is executed by the JVM.

When the JVM wants to actually run your byte code though, it needs to move the byte code into JVM memory. Historically, the JVM didn’t treat byte code differently from any other memory artifact, and so it just loaded the byte code into the JVM’s allocated memory space, just like a variable you allocate at runtime. Because byte code wasn’t expected to change by the JVM’s initial designers, their initial design point was that byte code would be loaded once and only once, and then never garbage collected so they put it in a special part of the memory space called PermGen … or Permanent Generation.

In early JVMs, PermGen was literally permanent and would never be garbage collected. A particularly large and complex java program with lots of byte code might need more PermGen than expected, and hence there were some startup flags that would let you change the size of the permgen space in the JVM, but by and large byte code in PermGen never changed, and was never garbage collected. Clever java developers though had figured out by that point though that they could generate new bytecode at *runtime* and hence you saw things like JSP, or Hibernate, or Apache Rhino where, at runtime, additional bytecode would get generated on the fly based on the state of the system.

You might think that these kinds of dynamic systems would increase the amount of byte code that existed, and hence require more and more PermGen, and you’d be correct. More subtly, in a lot of cases, these kinds of systems would end up recompiling the same, or very similar, bits of byte code over and over again. If you were developing in Glide for example, and changed a business rule, Rhino would compile the new version for you and stuff it in PermGen, but the old version would still be there as well. Rhino in fact, used to generate a fresh byte code representation of your javascript every time you ran it (by default at least) whether or not it changed. The net result of this is not only that you needed more PermGen, but that you had effective introduced a memory leak in PermGen.

The Java developers tried to fix this back in like java 1.3 by changing the garbage collector to start garbage collecting PermGen. Despite the name, it wasn’t really permanent anymore. The way they implemented this though was that they’d only touch PermGen on a full GC which was a reasonably rare event. So if you built up code in PermGen rapidly, you could still trigger an OOM specifically in the PermGen space just by outrunning the garbage collector, which wasn’t great. In the early days of glide, this was a pretty nasty problem we had to solve because we were constantly generating byte code on the fly and putting a mess of pressure on PermGen.

One of the first things we did to address this was to take control of our own byte code compilation. The first time we run a business rule inside of glide, we don’t natively call the Rhino execution engine. Instead we call some variation of a glide evaluate() method that eventually drops down into the Glide compiler class called, imaginatively, Compiler.java. The job of compiler, ironically, is to almost never compile anything. What it does is to keep a cache of stuff it has *previously* compiled and when you want to evaluate a script, it’ll look in the cache and it’ll almost always find a byte code representation there. In doing so we could ensure that we’d only compile a given expression once per JVM.

The compiler has some other logic in it as well that’s worth pointing out. One of the subtle things is that it will explicitly trigger a full garbage collection every 1000th time it compiles something. It does this to guard against the scenario I raised earlier where generating a lot of byte code in a hurry could fill up the JVM’s PermGen space before the normal garbage collector would kick in and trigger a full GC. In theory, the garbage collector should always do a full GC when it’s nearly out of memory, but empirically being nearly out of PermGen space doesn’t seem to trigger one, so we force it to GC every thousand compiles or so.

There are some other subtleties  to compilation worth talking about, specifically around short vs long expressions and redundancy. If you’ve worked in glide a lot, you’ve probably written a *lot* of little script fragments to determine how an application behaves or, alternately, adding expression logic to the system so the customer could use script to change how the system behaves. As a consequence there are tens and hundreds of redundant, but unique pieces of script in the platform for common expression like “true” or “false” or “gs.hasRole(‘admin’)”. These highly repetitive, and common, script fragments are a challenge for the compiler because they’ll fill up the expression cache and push out the much bigger compiled units for things like script includes. Additionally, due to the perverse nature of the rhino compiler, their compiled versions are like ten times larger than the scripts because of all the rhino overhead.

So to deal with these short, fragmentary, scripts, glide has a number of defenses. The first is a class called QuickEvaluator.java which tries to evaluate really simple script fragments without handing them off to rhino. If the script is simple and largely consists of chained expressions in the form foo().bar(‘something’).llama() QuickEvaluator can usually get the answer without ever invoking the Rhino compilation engine and hence without generating byte code. The second defense is that our compiler itself doesn’t actually compile short expressions. It turns out that for a short expression, there’s a lot of overhead in rhino’s byte code generation and execution to the extent that running a script in interpreted mode is faster than compiling it so our compiler will actually *interpret* rather than compile a short script and will avoid byte code generation entirely.

These days the modern JVM’s have changed the way they manage byte code. It’s no longer stored in normal JVM memory, but is instead stored in a new tranche of memory called MetaSpace. MetaSpace, in turn, doesn’t count against the JVM’s intrinsic memory limit, so if you allocate 2G to your JVM, precisely 0 MB will be consumed by MetaSpace. The flip side is that it’s still memory, and it still belongs to the JVM’s process and it is still allocated by the host operating system. So if you put a lot of un-necessary stuff into MetaSpace you will still cause memory problems, and hence all of the various defenses glide has in place to managed expression compilation are still extremely helpful in the new world of MetaSpace.

There are probably two different points to make here. One is that understanding how glide manages byte code generation, and management, will help you be a better glide programmer, and generally make you more effective at working with the system. The more macro point I do want to make though is that, especially if you come from a different engineering background, this may be the first job in your life where you had to even think about something like byte code management. After all, java is notionally a language which takes care of all of this stuff on your behalf, and most modern programming education positively encourages you to treat the JVM as an opaque, mysterious box to whom you delegate functions.

The reality though is that when you set out to write something as complex as glide you inevitably start by stringing together lots of pieces of well understood technology. Some of them are going to work great, but you are going to find some of them where the “normal” behavior isn’t doing what you need, and in those cases you’re going to have to dive in and really understand what the underlying subsystem is doing. That was the case here with java and byte code generation. It *almost* worked, but it took a lot of engineering effort on our part to make it totally work.

--- Pat

## Pat Casey on wizards and abstraction layers

When I was in my middle twenties, I decided that I wanted to learn how unix worked. I’d started by education in computers with old Atari computers, then graduated to dos, then windows. I’d even spent some time on early VMS systems during a summer internship, I’d done data entry on old 3270 terminal, and I’d shipped a couple of products that ran on HPUX or AIX, but I’d never really bothered to learn how the underlying operating system worked. This being the 1990s, that particular task was a lot easier than it would have been in the past because the various intel based (and free) linux distributions were starting to be available. So instead of having to spend several thousand dollars on a sun pizza box to run solaris, I could instead just take one of my old desktop PC’s and install a linux distro on it.

So, naturally, I asked around a bit and people I knew suggested that I should start with redhat (this predated fedora). So I dutifully went out and purchased all the redhat CD’s (if I remember it came on like 5 CD’s of physical media in a pre-broadband era), and tried installing it. It was, to be honest, something of a disaster. This was an era when redhat was pushing very hard on configuration wizards to hide the underlying unix system behind abstractions. So if you wanted to set up email, you ran the email setup wizard. If you wanted to set up an NFS server, you ran the package installation wizard and then you ran the NFS configuration wizard. There was, it seemed, a wizard for everything.

Only the wizards didn’t always work. I’d run through the email configuration wizard and then email would neither leave my computer bound for the internet nor arrive down in my local inbox. So I’d run the wizard a second time, and it still wouldn’t work. At which point there really wasn’t a whole lot I could do to try to fix things short of run the wizard a third time and start picking answers I believed to be incorrect in the hopes that by changing the input conditions on the wizard widely enough I’d eventually blunder into a configuration that worked which, unfortunately, I rarely did.

As a configuration tool, the wizards in the early redhat had two major problems, one related to implementation, and one structural. Implantation wise they were early generations of wizards, and they’d been written by people with a very deep understanding of technology. So the wizards, while a nice graphical abstraction, were still asking you very technical questions that you probably didn’t know the answer to. So the network setup wizard wanted, for example, my default gateway, and my netmask and didn’t even have the decency to guess that my netmask was probably 255.255.255.0. All in all, the wizards didn’t reduce the amount of information I needed to have in my head to successfully configure email, they just gave me a graphical way to fill in the information that they needed, and I didn’t have.

The other problem was more subtle, and that was that the wizards, by their nature, were an abstraction over the “real” configuration layer of the operating system. The actual smtp subsystem inside of redhat was driven by a mass of little text files with names like sendmail.cf. The wizards, in turn, would read/write these files on your behalf, but the underlying binaries were operating purely from the text configuration files. In this context, what the wizards did was to hide the configuration structure from the users in the name of simplifying it, only to fail at simplifying things.

What I eventually did in this particular case was uninstall red-hat from my machine and replace it instead with an early implementation of slackware. Slackware, at least back when I was using it, was a pretty polar opposite to redhat in that it didn’t have any graphical wizards. I don’t think it had one of the fancy linux gui’s either; if I recall it had an implementation of x-windows and you could run more than one terminal window at once, but if you were using slackware, you experienced unix as a lot of little terminal windows editing lots of little text files. For me at least, this was a much more approachable system because I felt like I had direct access to the guts of the system. Sendmail.cf, with all its little comments and structure, was easier for me to work with than the redhat configuration wizard.

That particular experience has always stuck with me as I looked at how we as technologist try to make technology more approachable to people who don’t understand the underlying system. Wizards are something of a default answer in that they can solve two problems. One is that they can make the mechanical act of inputting data easier by replacing, say, freeform text editing with structured combo boxes and radio buttons. The second is that they can replace a set of complicated technical questions with questions the user will actually know the answers to, and then translate their responses into the underlying technical grammar. If you can tick those two boxes, a wizard is going to help make your customer’s life easier. Absent the ability to do that though, a wizard is actually going to make your product harder to use.

The second, more macro, failure mode of wizards is that wizards are, by their nature structured and serial. They want to ask you information in a certain sequence, and then, based on your responses, prompt you to provide still more information. They encapsulate within themselves not just a data entry abstraction, but a strongly opinionated data entry sequence. For low volume tasks, this kind of structure can be helpful, since it guides the user through their first (or second) time entering the data, and present the user with information requests in small, but sized chunks rather than burying them under a mass of data. That’s why something like Turbo-Tax uses wizards extensively rather than just presenting you the raw underlying tax forms.

It’s worth pointing out though that an actual accountant, or at least a tax accountant, is going to operate faster on the raw tax form than via the Turbo-Tax wizards. These days most professional tax preparation people use some form of software support, but they tend to eschew the wizard paradigm and instead use variations on the digital form paradigm. They let the professional fill in information in whatever order works for them, validate it, and then print and submit the various tax forms once they are informationally complete. It’s generally a rule of thumb that wizards work best for things that people do infrequently, but are not an ideal paradigm for learned tasks where people want to operate in a less structured model.

The point of all this is really twofold. On a narrow level, I do want to make the concrete suggestion that you need to use wizards appropriately. For a wizard to give value to your customers it needs to tick three boxes. First of all it ask the customer questions that the customer will know the answer to. Second, it has to provide a data entry paradigm that’s discoverable and simple enough, to minimize the risk of data entry error. Finally, and most critically, the wizard ought to be aimed at low volume use cases, and shouldn’t become the only data entry paradigm for your application; experts don’t want/need that kind of structured interface.

More generally though, modern application design is all about abstracting away the computer internals from the end users. Even those little text files I liked in slackware were themselves a configuration abstraction over the memory structure that smtpd actually used at runtime, and the runtime was itself an abstraction over the underlying CPU/memory architecture of the machine. So one of the most common things we end up doing as engineers is building an abstraction over some other system, usually in the name of simplifying it. My advice on this front is not to fall into the trap of thinking that if task X is complicated, an abstraction atop task X will necessarily be simpler. The reality is that, to make it worth the complexity for you and your customer, an abstraction has to be *a lot* better than the underlying system, otherwise you are better off just leaving it be.

There is, in short, something of a precautionary principal we should apply when changing software, or, for that matter, changing anything. Just because something isn’t working well/is hard to use, does not mean that every possible alternate approach is going to be superior.

--- Pat

## Pat Casey on database backup

There’s a bit of a dark joke in the world of operations, which is that backing stuff up is easy, its *recovering* that’s hard. At one level this is true in that you can imagine a perfectly efficient backup system which backed up your data every night and promptly threw the backup copy away. *Technically* I’d be backing up your data, but when you actually asked me to recover any of it I wouldn’t be able to do so. In the real world of operations though, we need to deliver backup systems that both back up a set of data, and have the ability to get it back if we need it. So today we’re going to talk about backing up and recovering stuff in our datacenters, but we’re *not* going to talk about high availability except peripherally since that’s a whole different topic.

Conceptually, backing up the data on a computer is pretty easy. You take a complete copy of that data off of the source computer (the one you are backing up), and copy it to some other location, on a second computer or storage device. That way if the first computer ceases to exist for one reason or another, you can get a fresh new one out of a box somewhere, put it on the rack, and copy all the data back onto it, thus “recovering” your functionality. That kind of classical approach to backing up and recovering data is still more or less the norm for things like your laptop, but it’s rarely used these days in the operations world because, frankly, it doesn’t work if you try to apply it to an *active* dataset.

To explain why that’s such a problem, consider that in the world of ServiceNow, almost all of our customer data is stored in relational databases, specifically MariaDB. If you were to log into one of the MariaDB hosts and navigate through the file system, you’d actually see that the underlying data is, in fact, stored as a bunch of files in the file system. The way we run it, in fact, there’s going to be one physical file per physical table in existence, so there’s like a task.mdb, or a sys_user.mdb file on the file system. If the database isn’t running, you can, in fact, just copy those files someplace else, mount a new MariaDB server in front of them, and restart your process.

The problem emerges though when the database is in use. All of those files are being read from and written to more or less constantly, and it’ll take hours to copy them all off to some other location one at a time. I’d copy the first file at, say, noon, and the last file at like 4:00 PM. Even within the context of a specific file like task, which might notionally take me a half hour to copy off, the file itself would be changing as I copied it, such that the first set of bits I copied would be from the file as it existed at like 12:30, while the last set of bits would be from the file at like 1:00.

The net result of all of this is that, if I just naively copy all the files of a database out of the way, while its running, I get a deeply corrupted copy where the files are not internally consistent. I could, of course, shut my database down, copy all the files out of the way, and then start it up again, but on an enterprise application like ours it’s not exactly practical to tell our customers that we’re going to take their instance offline for a few hours every day to back it up, so we need another approach.

So in the datacenters, we actually use a specific backup tool (also from MariaDB) to back up the database. What it does, in turn, is pretty subtle in that it attaches to the database process itself and starts keeping track of all the changes it is making to the underlying files. Then, it starts copying the files out of the way, knowing full well they will be corrupted. Once it’s done copying the files out of the way, it detaches from the underlying database process and then stores the list of all the file system changes that took place during the backup window. To recover the data, it will reverse the process, first copying the (corrupted) file copies into place, and then re-applying all the various file system changes that it stored away in the first place.

It turns out that that’s not the *only* way you can back up a modern database though. One of the interesting characteristics of a modern database is that, while it will be making changes willy-nilly to its underlying physical file structure, it will always do so in a way that’s *recoverable* if, say, the power goes out. To achieve this particular trick, databases use something called a transaction log where they keep track of all the changes they need to make to the underlying file system along with periodic checkpoints where they can confirm that the changes have actually been made. In the event the database host crashes, the database will restart itself and can then do something called a recovery, by comparing all the changes it thinks it needed to make with the actual, underlying, physical files and re-applying anything that is missing.

What this means is that if there was a way to copy literally the entire file system all at once (including the transaction logs), you’d have something called a crash consistent copy (since it looks like the file system would look like after a crash). A modern database, in turn, can recover itself from a crash consistent copy so a perfect snapshot like that is, in fact, a valid backup strategy. So, what a lot of folks do these days is simply to snapshot the underlying file system upon which the database is running, and then copy that snapshot someplace else while letting the database happily run along.

With a couple of exceptions though, we don’t do that in our datacenters because of the performance impact of running with a snapshot enabled. There’s fundamentally nothing magic about snapshotting a file system. What happens behind the scenes is that the underlying file system starts keeping two logical views of the universe. In one view, the file system is continuing to change over time. In another view, the individual files are frozen in place at the time of the snapshot. In order to achieve this at the physical level (oversimplification) the file system will stash away a copy of every block of data it modifies so that, when it's viewing the snapshot, it has access to the *old* data, prior to the modification.

That kind of copy on modify approach means that the file system has to do double the work for any change in that it has to both write the new block of data *and* stash away the old block of data in such a way that the snapshot view of the file system still has access to it. It also has a subtle implication in that it requires extra physical storage to store all of those old data blocks, and the longer we keep the snapshot alive the more storage we are going to need to store all of those changed blocks.

In any event, with a couple of exceptions, we don’t use snapshots for backup because of the aforementioned performance implications while the snapshot is mounted. On some of our newer gear where we are using external storage arrays, the performance issue is less relevant, so we do use snapshots for various use cases there, but that’s a bit of a corner case rather than the norm. Virtually all the time, when we are backing something up in our datacenters, we are doing it via the MariaDB hot backup tool rather than via snapshotting.

That’s actually not the end of the story though in that finding a place to put all of those backup copies is itself a problem. We have on the order of 30PB of primary customer data that we back up, and we keep it for seven days. Between compressing the backups and doing differential backups a lot of the time, we really only need something like 100 PB of backup storage (I hope I’m remembering this right), which is less than 210PB, but still a prodigious amount of storage. The storage in question needs to be fast enough to actually stream the data in as fast as I can stream it off of the primary databases, and have a very fat network pipe to handle all the various backups going on at once.

Our older generation of datacenter architecture would solve this by storing the backups on other, different, database servers in the rack. So you’d have a database that was running on, say, server #2, and it needed a backup. It would ask the various other servers on its rack if anybody had any room for a backup, and if they did it would stream its backup across the local switch to that other server. That model was clever (at least I thought so), and worked well, but it did have a side effect that a particular customer’s backup could potentially be stored on a physical database server that was notionally running a different customer’s database, which made security and compliance at least a little nervous.

These days we pull a few database servers out of every rack and replace them with a storage array which gets used for the various backups. It’s a more traditional approach and, imho at least, less elegant, but it helps with the aforementioned security/compliance concerns. You could still imagine some sort of really bad failure though where, like a piano fell on the entire rack and destroyed not just the database servers, but also all of their backups. For production instances that’s still recoverable, because I have a whole other redundant version of their instance running in a different datacenter, but for a sub-production instance that would be a bad scenario.

So to guard against *that* scenario we actually store yet another copy of the backup on slow storage elsewhere in the datacenter. This second copy also has a protection in place such that literally nobody can delete the backups if they are less than seven days old which protects us against a scenario where a hacker, or a rogue administrator or employee decided they wanted to destroy a particular customer and all the backups. You could still do the job with a tactical nuclear missile or something that physically destroyed the facility, but a mere software attack would have a very hard time destroying that particular emergency set of backups. Hackers are industrious people of course, so it would be reckless of me or anybody else to claim it's literally impossible, but I think it’s safe to say it’s really hard.

The net of all this I suppose is that backing things up at datacenter scales is a non-trivial problem. It’s in that class of problems that seem pretty easy/obvious to a naive observer though, so if you happen to be one of the people who works on backups for a living you probably don’t get the respect you deserve from lay people. From my POV though it’s one of the most important things we do, and it’s a legitimately complex and interesting engineering problem.

Things get much harder, and much more interesting, at scale,

--- Pat